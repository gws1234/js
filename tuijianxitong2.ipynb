{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from tensorflow.python.ops import math_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import hashlib\n",
    "\n",
    "def _unzip(save_path, _, database_name, data_path):\n",
    "    \"\"\"\n",
    "    Unzip wrapper with the same interface as _ungzip\n",
    "    :param save_path: The path of the gzip files\n",
    "    :param database_name: Name of database\n",
    "    :param data_path: Path to extract to\n",
    "    :param _: HACK - Used to have to same interface as _ungzip\n",
    "    \"\"\"\n",
    "    print('Extracting {}...'.format(database_name))\n",
    "    with zipfile.ZipFile(save_path) as zf:\n",
    "        zf.extractall(data_path)\n",
    "\n",
    "def download_extract(database_name, data_path):\n",
    "    \"\"\"\n",
    "    Download and extract database\n",
    "    :param database_name: Database name\n",
    "    \"\"\"\n",
    "    DATASET_ML1M = 'ml-1m'\n",
    "\n",
    "    if database_name == DATASET_ML1M:\n",
    "        url = 'http://files.grouplens.org/datasets/movielens/ml-1m.zip'\n",
    "        hash_code = 'c4d9eecfca2ab87c1945afe126590906'\n",
    "        extract_path = os.path.join(data_path, 'ml-1m')\n",
    "        save_path = os.path.join(data_path, 'ml-1m.zip')\n",
    "        extract_fn = _unzip\n",
    "\n",
    "    if os.path.exists(extract_path):\n",
    "        print('Found {} Data'.format(database_name))\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Downloading {}'.format(database_name)) as pbar:\n",
    "            urlretrieve(\n",
    "                url,\n",
    "                save_path,\n",
    "                pbar.hook)\n",
    "\n",
    "    assert hashlib.md5(open(save_path, 'rb').read()).hexdigest() == hash_code, \\\n",
    "        '{} file is corrupted.  Remove the file and try again.'.format(save_path)\n",
    "\n",
    "    os.makedirs(extract_path)\n",
    "    try:\n",
    "        extract_fn(save_path, extract_path, database_name, data_path)\n",
    "    except Exception as err:\n",
    "        shutil.rmtree(extract_path)  # Remove extraction folder if there is an error\n",
    "        raise err\n",
    "\n",
    "    print('Done.')\n",
    "    # Remove compressed data\n",
    "#     os.remove(save_path)\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    \"\"\"\n",
    "    Handle Progress Bar while Downloading\n",
    "    \"\"\"\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        \"\"\"\n",
    "        A hook function that will be called once on establishment of the network connection and\n",
    "        once after each block read thereafter.\n",
    "        :param block_num: A count of blocks transferred so far\n",
    "        :param block_size: Block size in bytes\n",
    "        :param total_size: The total size of the file. This may be -1 on older FTP servers which do not return\n",
    "                            a file size in response to a retrieval request.\n",
    "        \"\"\"\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ml-1m Data\n"
     ]
    }
   ],
   "source": [
    "data_dir = './'\n",
    "download_extract('ml-1m', data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>OccupationID</th>\n",
       "      <th>Zip-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID Gender  Age  OccupationID Zip-code\n",
       "0       1      F    1            10    48067\n",
       "1       2      M   56            16    70072\n",
       "2       3      M   25            15    55117\n",
       "3       4      M   45             7    02460\n",
       "4       5      M   25            20    55455"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_title = ['UserID', 'Gender', 'Age', 'OccupationID', 'Zip-code']\n",
    "users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                               Title                        Genres\n",
       "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4        5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_title = ['MovieID', 'Title', 'Genres']\n",
    "movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  timestamps\n",
       "0       1     1193       5   978300760\n",
       "1       1      661       3   978302109\n",
       "2       1      914       3   978301968\n",
       "3       1     3408       4   978300275\n",
       "4       1     2355       5   978824291"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_title = ['UserID','MovieID', 'Rating', 'timestamps']\n",
    "ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    #读取User数据\n",
    "    users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']\n",
    "    users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "    users = users.filter(regex='UserID|Gender|Age|JobID')\n",
    "    users_orig = users.values\n",
    "    #改变User数据中性别和年龄\n",
    "    gender_map = {'F':0, 'M':1}\n",
    "    users['Gender'] = users['Gender'].map(gender_map)\n",
    "\n",
    "    age_map = {val:ii for ii,val in enumerate(set(users['Age']))}\n",
    "    users['Age'] = users['Age'].map(age_map)\n",
    "\n",
    "    #读取Movie数据集\n",
    "    movies_title = ['MovieID', 'Title', 'Genres']\n",
    "    movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "    movies_orig = movies.values\n",
    "    #将Title中的年份去掉\n",
    "    pattern = re.compile(r'^(.*)\\((\\d+)\\)$')\n",
    "\n",
    "    title_map = {val:pattern.match(val).group(1) for ii,val in enumerate(set(movies['Title']))}\n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "    #电影类型转数字字典\n",
    "    genres_set = set()\n",
    "    for val in movies['Genres'].str.split('|'):\n",
    "        genres_set.update(val)\n",
    "\n",
    "    genres_set.add('<PAD>')\n",
    "    genres2int = {val:ii for ii, val in enumerate(genres_set)}\n",
    "\n",
    "    #将电影类型转成等长数字列表，长度是18\n",
    "    genres_map = {val:[genres2int[row] for row in val.split('|')] for ii,val in enumerate(set(movies['Genres']))}\n",
    "\n",
    "    for key in genres_map:\n",
    "        for cnt in range(max(genres2int.values()) - len(genres_map[key])):\n",
    "            genres_map[key].insert(len(genres_map[key]) + cnt,genres2int['<PAD>'])\n",
    "    \n",
    "    movies['Genres'] = movies['Genres'].map(genres_map)\n",
    "\n",
    "    #电影Title转数字字典\n",
    "    title_set = set()\n",
    "    for val in movies['Title'].str.split():\n",
    "        title_set.update(val)\n",
    "    \n",
    "    title_set.add('<PAD>')\n",
    "    title2int = {val:ii for ii, val in enumerate(title_set)}\n",
    "\n",
    "    #将电影Title转成等长数字列表，长度是15\n",
    "    title_count = 15\n",
    "    title_map = {val:[title2int[row] for row in val.split()] for ii,val in enumerate(set(movies['Title']))}\n",
    "    \n",
    "    for key in title_map:\n",
    "        for cnt in range(title_count - len(title_map[key])):\n",
    "            title_map[key].insert(len(title_map[key]) + cnt,title2int['<PAD>'])\n",
    "    \n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "    #读取评分数据集\n",
    "    ratings_title = ['UserID','MovieID', 'ratings', 'timestamps']\n",
    "    ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "    ratings = ratings.filter(regex='UserID|MovieID|ratings')\n",
    "\n",
    "    #合并三个表\n",
    "    data = pd.merge(pd.merge(ratings, users), movies)\n",
    "    \n",
    "    #将数据分成X和y两张表\n",
    "    target_fields = ['ratings']\n",
    "    features_pd, targets_pd = data.drop(target_fields, axis=1), data[target_fields]\n",
    "    \n",
    "    features = features_pd.values\n",
    "    targets_values = targets_pd.values\n",
    "    \n",
    "    return title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  import sys\n",
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:64: FutureWarning: read_table is deprecated, use read_csv instead.\n"
     ]
    }
   ],
   "source": [
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_data()\n",
    "\n",
    "pickle.dump((title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>JobID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  Gender  Age  JobID\n",
       "0       1       0    0     10\n",
       "1       2       1    5     16\n",
       "2       3       1    6     15\n",
       "3       4       1    2      7\n",
       "4       5       1    6     20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[1826, 3814, 2500, 2500, 2500, 2500, 2500, 250...</td>\n",
       "      <td>[8, 18, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[3312, 2500, 2500, 2500, 2500, 2500, 2500, 250...</td>\n",
       "      <td>[9, 18, 5, 16, 16, 16, 16, 16, 16, 16, 16, 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[2009, 99, 4302, 2500, 2500, 2500, 2500, 2500,...</td>\n",
       "      <td>[15, 7, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[2514, 4284, 571, 2500, 2500, 2500, 2500, 2500...</td>\n",
       "      <td>[15, 12, 16, 16, 16, 16, 16, 16, 16, 16, 16, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[5044, 4131, 2903, 322, 1463, 570, 2500, 2500,...</td>\n",
       "      <td>[15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                                              Title  \\\n",
       "0        1  [1826, 3814, 2500, 2500, 2500, 2500, 2500, 250...   \n",
       "1        2  [3312, 2500, 2500, 2500, 2500, 2500, 2500, 250...   \n",
       "2        3  [2009, 99, 4302, 2500, 2500, 2500, 2500, 2500,...   \n",
       "3        4  [2514, 4284, 571, 2500, 2500, 2500, 2500, 2500...   \n",
       "4        5  [5044, 4131, 2903, 322, 1463, 570, 2500, 2500,...   \n",
       "\n",
       "                                              Genres  \n",
       "0  [8, 18, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16...  \n",
       "1  [9, 18, 5, 16, 16, 16, 16, 16, 16, 16, 16, 16,...  \n",
       "2  [15, 7, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16...  \n",
       "3  [15, 12, 16, 16, 16, 16, 16, 16, 16, 16, 16, 1...  \n",
       "4  [15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 1...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1,\n",
       "       list([1826, 3814, 2500, 2500, 2500, 2500, 2500, 2500, 2500, 2500, 2500, 2500, 2500, 2500, 2500]),\n",
       "       list([8, 18, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    Save parameters to file\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    \"\"\"\n",
    "    Load parameters from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open('params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#嵌入矩阵的维度\n",
    "embed_dim = 32\n",
    "#用户ID个数\n",
    "uid_max = max(features.take(0,1)) + 1 # 6040\n",
    "#性别个数\n",
    "gender_max = max(features.take(2,1)) + 1 # 1 + 1 = 2\n",
    "#年龄类别个数\n",
    "age_max = max(features.take(3,1)) + 1 # 6 + 1 = 7\n",
    "#职业个数\n",
    "job_max = max(features.take(4,1)) + 1# 20 + 1 = 21\n",
    "\n",
    "#电影ID个数\n",
    "movie_id_max = max(features.take(1,1)) + 1 # 3952\n",
    "#电影类型个数\n",
    "movie_categories_max = max(genres2int.values()) + 1 # 18 + 1 = 19\n",
    "#电影名单词个数\n",
    "movie_title_max = len(title_set) # 5216\n",
    "\n",
    "#对电影类型嵌入向量做加和操作的标志，考虑过使用mean做平均，但是没实现mean\n",
    "combiner = \"sum\"\n",
    "\n",
    "#电影名长度\n",
    "sentences_size = title_count # = 15\n",
    "#文本卷积滑动窗口，分别滑动2, 3, 4, 5个单词\n",
    "window_sizes = {2, 3, 4, 5}\n",
    "#文本卷积核数量\n",
    "filter_num = 8\n",
    "\n",
    "#电影ID转下标的字典，数据集中电影ID跟下标不一致，比如第5行的数据电影ID不一定是5\n",
    "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 5\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "\n",
    "dropout_keep = 0.5\n",
    "# Learning Rate\n",
    "learning_rate = 0.0001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 20\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    uid = tf.placeholder(tf.int32, [None, 1], name=\"uid\")\n",
    "    user_gender = tf.placeholder(tf.int32, [None, 1], name=\"user_gender\")\n",
    "    user_age = tf.placeholder(tf.int32, [None, 1], name=\"user_age\")\n",
    "    user_job = tf.placeholder(tf.int32, [None, 1], name=\"user_job\")\n",
    "    \n",
    "    movie_id = tf.placeholder(tf.int32, [None, 1], name=\"movie_id\")\n",
    "    movie_categories = tf.placeholder(tf.int32, [None, 18], name=\"movie_categories\")\n",
    "    movie_titles = tf.placeholder(tf.int32, [None, 15], name=\"movie_titles\")\n",
    "    targets = tf.placeholder(tf.int32, [None, 1], name=\"targets\")\n",
    "    LearningRate = tf.placeholder(tf.float32, name = \"LearningRate\")\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name = \"dropout_keep_prob\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, LearningRate, dropout_keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_embedding(uid, user_gender, user_age, user_job):\n",
    "    with tf.name_scope(\"user_embedding\"):\n",
    "        uid_embed_matrix = tf.Variable(tf.random_uniform([uid_max, embed_dim], -1, 1), name = \"uid_embed_matrix\")\n",
    "        uid_embed_layer = tf.nn.embedding_lookup(uid_embed_matrix, uid, name = \"uid_embed_layer\")\n",
    "    \n",
    "        gender_embed_matrix = tf.Variable(tf.random_uniform([gender_max, embed_dim // 2], -1, 1), name= \"gender_embed_matrix\")\n",
    "        gender_embed_layer = tf.nn.embedding_lookup(gender_embed_matrix, user_gender, name = \"gender_embed_layer\")\n",
    "        \n",
    "        age_embed_matrix = tf.Variable(tf.random_uniform([age_max, embed_dim // 2], -1, 1), name=\"age_embed_matrix\")\n",
    "        age_embed_layer = tf.nn.embedding_lookup(age_embed_matrix, user_age, name=\"age_embed_layer\")\n",
    "        \n",
    "        job_embed_matrix = tf.Variable(tf.random_uniform([job_max, embed_dim // 2], -1, 1), name = \"job_embed_matrix\")\n",
    "        job_embed_layer = tf.nn.embedding_lookup(job_embed_matrix, user_job, name = \"job_embed_layer\")\n",
    "    return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
    "    with tf.name_scope(\"user_fc\"):\n",
    "        #第一层全连接\n",
    "        uid_fc_layer = tf.layers.dense(uid_embed_layer, embed_dim, name = \"uid_fc_layer\", activation=tf.nn.relu)\n",
    "        gender_fc_layer = tf.layers.dense(gender_embed_layer, embed_dim, name = \"gender_fc_layer\", activation=tf.nn.relu)\n",
    "        age_fc_layer = tf.layers.dense(age_embed_layer, embed_dim, name =\"age_fc_layer\", activation=tf.nn.relu)\n",
    "        job_fc_layer = tf.layers.dense(job_embed_layer, embed_dim, name = \"job_fc_layer\", activation=tf.nn.relu)\n",
    "        \n",
    "        #第二层全连接\n",
    "        user_combine_layer = tf.concat([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)  #(?, 1, 128)\n",
    "        user_combine_layer = tf.contrib.layers.fully_connected(user_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "    \n",
    "        user_combine_layer_flat = tf.reshape(user_combine_layer, [-1, 200])\n",
    "    return user_combine_layer, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_id_embed_layer(movie_id):\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        movie_id_embed_matrix = tf.Variable(tf.random_uniform([movie_id_max, embed_dim], -1, 1), name = \"movie_id_embed_matrix\")\n",
    "        movie_id_embed_layer = tf.nn.embedding_lookup(movie_id_embed_matrix, movie_id, name = \"movie_id_embed_layer\")\n",
    "    return movie_id_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_categories_layers(movie_categories):\n",
    "    with tf.name_scope(\"movie_categories_layers\"):\n",
    "        movie_categories_embed_matrix = tf.Variable(tf.random_uniform([movie_categories_max, embed_dim], -1, 1), name = \"movie_categories_embed_matrix\")\n",
    "        movie_categories_embed_layer = tf.nn.embedding_lookup(movie_categories_embed_matrix, movie_categories, name = \"movie_categories_embed_layer\")\n",
    "        if combiner == \"sum\":\n",
    "            movie_categories_embed_layer = tf.reduce_sum(movie_categories_embed_layer, axis=1, keep_dims=True)\n",
    "    #     elif combiner == \"mean\":\n",
    "\n",
    "    return movie_categories_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_cnn_layer(movie_titles):\n",
    "    #从嵌入矩阵中得到电影名对应的各个单词的嵌入向量\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        movie_title_embed_matrix = tf.Variable(tf.random_uniform([movie_title_max, embed_dim], -1, 1), name = \"movie_title_embed_matrix\")\n",
    "        movie_title_embed_layer = tf.nn.embedding_lookup(movie_title_embed_matrix, movie_titles, name = \"movie_title_embed_layer\")\n",
    "        movie_title_embed_layer_expand = tf.expand_dims(movie_title_embed_layer, -1)\n",
    "    \n",
    "    #对文本嵌入层使用不同尺寸的卷积核做卷积和最大池化\n",
    "    pool_layer_lst = []\n",
    "    for window_size in window_sizes:\n",
    "        with tf.name_scope(\"movie_txt_conv_maxpool_{}\".format(window_size)):\n",
    "            filter_weights = tf.Variable(tf.truncated_normal([window_size, embed_dim, 1, filter_num],stddev=0.1),name = \"filter_weights\")\n",
    "            filter_bias = tf.Variable(tf.constant(0.1, shape=[filter_num]), name=\"filter_bias\")\n",
    "            \n",
    "            conv_layer = tf.nn.conv2d(movie_title_embed_layer_expand, filter_weights, [1,1,1,1], padding=\"VALID\", name=\"conv_layer\")\n",
    "            relu_layer = tf.nn.relu(tf.nn.bias_add(conv_layer,filter_bias), name =\"relu_layer\")\n",
    "            \n",
    "            maxpool_layer = tf.nn.max_pool(relu_layer, [1,sentences_size - window_size + 1 ,1,1], [1,1,1,1], padding=\"VALID\", name=\"maxpool_layer\")\n",
    "            pool_layer_lst.append(maxpool_layer)\n",
    "\n",
    "    #Dropout层\n",
    "    with tf.name_scope(\"pool_dropout\"):\n",
    "        pool_layer = tf.concat(pool_layer_lst, 3, name =\"pool_layer\")\n",
    "        max_num = len(window_sizes) * filter_num\n",
    "        pool_layer_flat = tf.reshape(pool_layer , [-1, 1, max_num], name = \"pool_layer_flat\")\n",
    "    \n",
    "        dropout_layer = tf.nn.dropout(pool_layer_flat, dropout_keep_prob, name = \"dropout_layer\")\n",
    "    return pool_layer_flat, dropout_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_feature_layer(movie_id_embed_layer, movie_categories_embed_layer, dropout_layer):\n",
    "    with tf.name_scope(\"movie_fc\"):\n",
    "        #第一层全连接\n",
    "        movie_id_fc_layer = tf.layers.dense(movie_id_embed_layer, embed_dim, name = \"movie_id_fc_layer\", activation=tf.nn.relu)\n",
    "        movie_categories_fc_layer = tf.layers.dense(movie_categories_embed_layer, embed_dim, name = \"movie_categories_fc_layer\", activation=tf.nn.relu)\n",
    "    \n",
    "        #第二层全连接\n",
    "        movie_combine_layer = tf.concat([movie_id_fc_layer, movie_categories_fc_layer, dropout_layer], 2)  #(?, 1, 96)\n",
    "        movie_combine_layer = tf.contrib.layers.fully_connected(movie_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "    \n",
    "        movie_combine_layer_flat = tf.reshape(movie_combine_layer, [-1, 200])\n",
    "    return movie_combine_layer, movie_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-19-dc08624e5fcc>:4: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-21-559a1ee9ce9e>:6: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From <ipython-input-22-bb012f2abe28>:27: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    #获取输入占位符\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob = get_inputs()\n",
    "    #获取User的4个嵌入向量\n",
    "    uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = get_user_embedding(uid, user_gender, user_age, user_job)\n",
    "    #得到用户特征\n",
    "    user_combine_layer, user_combine_layer_flat = get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer)\n",
    "    #获取电影ID的嵌入向量\n",
    "    movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
    "    #获取电影类型的嵌入向量\n",
    "    movie_categories_embed_layer = get_movie_categories_layers(movie_categories)\n",
    "    #获取电影名的特征向量\n",
    "    pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles)\n",
    "    #得到电影特征\n",
    "    movie_combine_layer, movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer, \n",
    "                                                                                movie_categories_embed_layer, \n",
    "                                                                                dropout_layer)\n",
    "    #计算出评分，要注意两个不同的方案，inference的名字（name值）是不一样的，后面做推荐时要根据name取得tensor\n",
    "    with tf.name_scope(\"inference\"):\n",
    "        #将用户特征和电影特征作为输入，经过全连接，输出一个值的方案\n",
    "#         inference_layer = tf.concat([user_combine_layer_flat, movie_combine_layer_flat], 1)  #(?, 200)\n",
    "#         inference = tf.layers.dense(inference_layer, 1,\n",
    "#                                     kernel_initializer=tf.truncated_normal_initializer(stddev=0.01), \n",
    "#                                     kernel_regularizer=tf.nn.l2_loss, name=\"inference\")\n",
    "        #简单的将用户特征和电影特征做矩阵乘法得到一个预测评分\n",
    "#        inference = tf.matmul(user_combine_layer_flat, tf.transpose(movie_combine_layer_flat))\n",
    "        inference = tf.reduce_sum(user_combine_layer_flat * movie_combine_layer_flat, axis=1)\n",
    "        inference = tf.expand_dims(inference, axis=1)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # MSE损失，将计算值回归到评分\n",
    "        cost = tf.losses.mean_squared_error(targets, inference )\n",
    "        loss = tf.reduce_mean(cost)\n",
    "    # 优化损失 \n",
    "#     train_op = tf.train.AdamOptimizer(lr).minimize(loss)  #cost\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    gradients = optimizer.compute_gradients(loss)  #cost\n",
    "    train_op = optimizer.apply_gradients(gradients, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'inference/ExpandDims:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to C:\\Users\\·戴\\runs\\1556884183\n",
      "\n",
      "2019-05-03T19:49:52.322510: Epoch   0 Batch    0/3125   train_loss = 62.209\n",
      "2019-05-03T19:49:54.095144: Epoch   0 Batch   20/3125   train_loss = 9.200\n",
      "2019-05-03T19:49:55.887371: Epoch   0 Batch   40/3125   train_loss = 5.468\n",
      "2019-05-03T19:49:57.683230: Epoch   0 Batch   60/3125   train_loss = 4.376\n",
      "2019-05-03T19:49:59.464437: Epoch   0 Batch   80/3125   train_loss = 3.192\n",
      "2019-05-03T19:50:01.270365: Epoch   0 Batch  100/3125   train_loss = 3.037\n",
      "2019-05-03T19:50:03.111891: Epoch   0 Batch  120/3125   train_loss = 2.562\n",
      "2019-05-03T19:50:05.018136: Epoch   0 Batch  140/3125   train_loss = 2.728\n",
      "2019-05-03T19:50:07.283114: Epoch   0 Batch  160/3125   train_loss = 1.631\n",
      "2019-05-03T19:50:09.584133: Epoch   0 Batch  180/3125   train_loss = 1.980\n",
      "2019-05-03T19:50:11.854764: Epoch   0 Batch  200/3125   train_loss = 2.033\n",
      "2019-05-03T19:50:14.146274: Epoch   0 Batch  220/3125   train_loss = 1.814\n",
      "2019-05-03T19:50:16.417387: Epoch   0 Batch  240/3125   train_loss = 1.653\n",
      "2019-05-03T19:50:18.649766: Epoch   0 Batch  260/3125   train_loss = 1.969\n",
      "2019-05-03T19:50:20.917579: Epoch   0 Batch  280/3125   train_loss = 1.832\n",
      "2019-05-03T19:50:23.159664: Epoch   0 Batch  300/3125   train_loss = 1.727\n",
      "2019-05-03T19:50:25.420022: Epoch   0 Batch  320/3125   train_loss = 1.856\n",
      "2019-05-03T19:50:27.684828: Epoch   0 Batch  340/3125   train_loss = 1.664\n",
      "2019-05-03T19:50:29.939004: Epoch   0 Batch  360/3125   train_loss = 1.777\n",
      "2019-05-03T19:50:32.181929: Epoch   0 Batch  380/3125   train_loss = 1.594\n",
      "2019-05-03T19:50:34.444685: Epoch   0 Batch  400/3125   train_loss = 1.567\n",
      "2019-05-03T19:50:36.676996: Epoch   0 Batch  420/3125   train_loss = 1.477\n",
      "2019-05-03T19:50:38.938340: Epoch   0 Batch  440/3125   train_loss = 1.648\n",
      "2019-05-03T19:50:41.179272: Epoch   0 Batch  460/3125   train_loss = 1.824\n",
      "2019-05-03T19:50:43.451686: Epoch   0 Batch  480/3125   train_loss = 1.551\n",
      "2019-05-03T19:50:45.776920: Epoch   0 Batch  500/3125   train_loss = 1.372\n",
      "2019-05-03T19:50:48.146858: Epoch   0 Batch  520/3125   train_loss = 1.441\n",
      "2019-05-03T19:50:50.380309: Epoch   0 Batch  540/3125   train_loss = 1.380\n",
      "2019-05-03T19:50:52.662070: Epoch   0 Batch  560/3125   train_loss = 1.689\n",
      "2019-05-03T19:50:54.932248: Epoch   0 Batch  580/3125   train_loss = 1.540\n",
      "2019-05-03T19:50:57.175456: Epoch   0 Batch  600/3125   train_loss = 1.638\n",
      "2019-05-03T19:50:59.417142: Epoch   0 Batch  620/3125   train_loss = 1.654\n",
      "2019-05-03T19:51:01.686464: Epoch   0 Batch  640/3125   train_loss = 1.464\n",
      "2019-05-03T19:51:04.004710: Epoch   0 Batch  660/3125   train_loss = 1.380\n",
      "2019-05-03T19:51:06.245291: Epoch   0 Batch  680/3125   train_loss = 1.397\n",
      "2019-05-03T19:51:08.495984: Epoch   0 Batch  700/3125   train_loss = 1.328\n",
      "2019-05-03T19:51:10.719541: Epoch   0 Batch  720/3125   train_loss = 1.282\n",
      "2019-05-03T19:51:12.960226: Epoch   0 Batch  740/3125   train_loss = 1.585\n",
      "2019-05-03T19:51:15.214389: Epoch   0 Batch  760/3125   train_loss = 1.449\n",
      "2019-05-03T19:51:17.477235: Epoch   0 Batch  780/3125   train_loss = 1.479\n",
      "2019-05-03T19:51:19.750026: Epoch   0 Batch  800/3125   train_loss = 1.316\n",
      "2019-05-03T19:51:22.002139: Epoch   0 Batch  820/3125   train_loss = 1.397\n",
      "2019-05-03T19:51:24.251824: Epoch   0 Batch  840/3125   train_loss = 1.265\n",
      "2019-05-03T19:51:26.515508: Epoch   0 Batch  860/3125   train_loss = 1.206\n",
      "2019-05-03T19:51:28.765487: Epoch   0 Batch  880/3125   train_loss = 1.269\n",
      "2019-05-03T19:51:31.005903: Epoch   0 Batch  900/3125   train_loss = 1.251\n",
      "2019-05-03T19:51:33.310979: Epoch   0 Batch  920/3125   train_loss = 1.358\n",
      "2019-05-03T19:51:35.583556: Epoch   0 Batch  940/3125   train_loss = 1.519\n",
      "2019-05-03T19:51:37.826157: Epoch   0 Batch  960/3125   train_loss = 1.476\n",
      "2019-05-03T19:51:40.088624: Epoch   0 Batch  980/3125   train_loss = 1.486\n",
      "2019-05-03T19:51:42.369138: Epoch   0 Batch 1000/3125   train_loss = 1.279\n",
      "2019-05-03T19:51:44.680920: Epoch   0 Batch 1020/3125   train_loss = 1.498\n",
      "2019-05-03T19:51:46.921941: Epoch   0 Batch 1040/3125   train_loss = 1.201\n",
      "2019-05-03T19:51:49.163015: Epoch   0 Batch 1060/3125   train_loss = 1.472\n",
      "2019-05-03T19:51:51.394298: Epoch   0 Batch 1080/3125   train_loss = 1.262\n",
      "2019-05-03T19:51:53.657378: Epoch   0 Batch 1100/3125   train_loss = 1.462\n",
      "2019-05-03T19:51:55.919821: Epoch   0 Batch 1120/3125   train_loss = 1.427\n",
      "2019-05-03T19:51:58.179829: Epoch   0 Batch 1140/3125   train_loss = 1.370\n",
      "2019-05-03T19:52:00.451395: Epoch   0 Batch 1160/3125   train_loss = 1.374\n",
      "2019-05-03T19:52:02.691934: Epoch   0 Batch 1180/3125   train_loss = 1.370\n",
      "2019-05-03T19:52:05.006207: Epoch   0 Batch 1200/3125   train_loss = 1.363\n",
      "2019-05-03T19:52:07.328223: Epoch   0 Batch 1220/3125   train_loss = 1.226\n",
      "2019-05-03T19:52:09.626762: Epoch   0 Batch 1240/3125   train_loss = 1.151\n",
      "2019-05-03T19:52:11.879452: Epoch   0 Batch 1260/3125   train_loss = 1.241\n",
      "2019-05-03T19:52:14.149146: Epoch   0 Batch 1280/3125   train_loss = 1.265\n",
      "2019-05-03T19:52:16.412008: Epoch   0 Batch 1300/3125   train_loss = 1.340\n",
      "2019-05-03T19:52:18.668037: Epoch   0 Batch 1320/3125   train_loss = 1.315\n",
      "2019-05-03T19:52:20.928564: Epoch   0 Batch 1340/3125   train_loss = 1.117\n",
      "2019-05-03T19:52:23.178538: Epoch   0 Batch 1360/3125   train_loss = 1.114\n",
      "2019-05-03T19:52:25.451851: Epoch   0 Batch 1380/3125   train_loss = 1.196\n",
      "2019-05-03T19:52:27.705773: Epoch   0 Batch 1400/3125   train_loss = 1.276\n",
      "2019-05-03T19:52:29.966752: Epoch   0 Batch 1420/3125   train_loss = 1.233\n",
      "2019-05-03T19:52:32.226434: Epoch   0 Batch 1440/3125   train_loss = 1.124\n",
      "2019-05-03T19:52:34.490992: Epoch   0 Batch 1460/3125   train_loss = 1.324\n",
      "2019-05-03T19:52:36.763538: Epoch   0 Batch 1480/3125   train_loss = 1.248\n",
      "2019-05-03T19:52:39.119305: Epoch   0 Batch 1500/3125   train_loss = 1.454\n",
      "2019-05-03T19:52:41.399088: Epoch   0 Batch 1520/3125   train_loss = 1.257\n",
      "2019-05-03T19:52:43.781737: Epoch   0 Batch 1540/3125   train_loss = 1.338\n",
      "2019-05-03T19:52:46.012866: Epoch   0 Batch 1560/3125   train_loss = 1.288\n",
      "2019-05-03T19:52:48.273824: Epoch   0 Batch 1580/3125   train_loss = 1.268\n",
      "2019-05-03T19:52:50.536798: Epoch   0 Batch 1600/3125   train_loss = 1.295\n",
      "2019-05-03T19:52:52.817013: Epoch   0 Batch 1620/3125   train_loss = 1.305\n",
      "2019-05-03T19:52:55.082432: Epoch   0 Batch 1640/3125   train_loss = 1.432\n",
      "2019-05-03T19:52:57.331521: Epoch   0 Batch 1660/3125   train_loss = 1.359\n",
      "2019-05-03T19:52:59.602555: Epoch   0 Batch 1680/3125   train_loss = 1.332\n",
      "2019-05-03T19:53:01.855551: Epoch   0 Batch 1700/3125   train_loss = 1.084\n",
      "2019-05-03T19:53:04.118704: Epoch   0 Batch 1720/3125   train_loss = 1.224\n",
      "2019-05-03T19:53:06.430908: Epoch   0 Batch 1740/3125   train_loss = 1.259\n",
      "2019-05-03T19:53:08.743022: Epoch   0 Batch 1760/3125   train_loss = 1.394\n",
      "2019-05-03T19:53:11.035371: Epoch   0 Batch 1780/3125   train_loss = 1.216\n",
      "2019-05-03T19:53:13.294301: Epoch   0 Batch 1800/3125   train_loss = 1.220\n",
      "2019-05-03T19:53:15.566466: Epoch   0 Batch 1820/3125   train_loss = 1.296\n",
      "2019-05-03T19:53:17.825319: Epoch   0 Batch 1840/3125   train_loss = 1.292\n",
      "2019-05-03T19:53:20.087824: Epoch   0 Batch 1860/3125   train_loss = 1.246\n",
      "2019-05-03T19:53:22.349230: Epoch   0 Batch 1880/3125   train_loss = 1.205\n",
      "2019-05-03T19:53:24.628575: Epoch   0 Batch 1900/3125   train_loss = 1.060\n",
      "2019-05-03T19:53:26.914135: Epoch   0 Batch 1920/3125   train_loss = 1.147\n",
      "2019-05-03T19:53:29.176560: Epoch   0 Batch 1940/3125   train_loss = 1.186\n",
      "2019-05-03T19:53:31.447252: Epoch   0 Batch 1960/3125   train_loss = 1.099\n",
      "2019-05-03T19:53:33.719608: Epoch   0 Batch 1980/3125   train_loss = 1.114\n",
      "2019-05-03T19:53:36.011117: Epoch   0 Batch 2000/3125   train_loss = 1.433\n",
      "2019-05-03T19:53:38.290402: Epoch   0 Batch 2020/3125   train_loss = 1.298\n",
      "2019-05-03T19:53:40.534445: Epoch   0 Batch 2040/3125   train_loss = 1.104\n",
      "2019-05-03T19:53:42.787145: Epoch   0 Batch 2060/3125   train_loss = 1.109\n",
      "2019-05-03T19:53:45.126652: Epoch   0 Batch 2080/3125   train_loss = 1.197\n",
      "2019-05-03T19:53:47.400194: Epoch   0 Batch 2100/3125   train_loss = 1.179\n",
      "2019-05-03T19:53:49.688138: Epoch   0 Batch 2120/3125   train_loss = 1.093\n",
      "2019-05-03T19:53:52.025500: Epoch   0 Batch 2140/3125   train_loss = 1.239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-03T19:53:54.297908: Epoch   0 Batch 2160/3125   train_loss = 1.227\n",
      "2019-05-03T19:53:56.605952: Epoch   0 Batch 2180/3125   train_loss = 1.107\n",
      "2019-05-03T19:53:58.912069: Epoch   0 Batch 2200/3125   train_loss = 1.166\n",
      "2019-05-03T19:54:01.175790: Epoch   0 Batch 2220/3125   train_loss = 1.139\n",
      "2019-05-03T19:54:03.427128: Epoch   0 Batch 2240/3125   train_loss = 1.079\n",
      "2019-05-03T19:54:05.699703: Epoch   0 Batch 2260/3125   train_loss = 1.095\n",
      "2019-05-03T19:54:07.991341: Epoch   0 Batch 2280/3125   train_loss = 1.249\n",
      "2019-05-03T19:54:10.302864: Epoch   0 Batch 2300/3125   train_loss = 1.368\n",
      "2019-05-03T19:54:12.565208: Epoch   0 Batch 2320/3125   train_loss = 1.339\n",
      "2019-05-03T19:54:14.859207: Epoch   0 Batch 2340/3125   train_loss = 1.278\n",
      "2019-05-03T19:54:17.110564: Epoch   0 Batch 2360/3125   train_loss = 1.197\n",
      "2019-05-03T19:54:19.393953: Epoch   0 Batch 2380/3125   train_loss = 1.260\n",
      "2019-05-03T19:54:21.656733: Epoch   0 Batch 2400/3125   train_loss = 1.291\n",
      "2019-05-03T19:54:23.918258: Epoch   0 Batch 2420/3125   train_loss = 1.249\n",
      "2019-05-03T19:54:26.200374: Epoch   0 Batch 2440/3125   train_loss = 1.213\n",
      "2019-05-03T19:54:28.494987: Epoch   0 Batch 2460/3125   train_loss = 1.168\n",
      "2019-05-03T19:54:30.827860: Epoch   0 Batch 2480/3125   train_loss = 1.277\n",
      "2019-05-03T19:54:33.099609: Epoch   0 Batch 2500/3125   train_loss = 1.235\n",
      "2019-05-03T19:54:35.372016: Epoch   0 Batch 2520/3125   train_loss = 1.084\n",
      "2019-05-03T19:54:37.649598: Epoch   0 Batch 2540/3125   train_loss = 1.101\n",
      "2019-05-03T19:54:40.057000: Epoch   0 Batch 2560/3125   train_loss = 0.964\n",
      "2019-05-03T19:54:42.325619: Epoch   0 Batch 2580/3125   train_loss = 1.123\n",
      "2019-05-03T19:54:44.588085: Epoch   0 Batch 2600/3125   train_loss = 1.189\n",
      "2019-05-03T19:54:46.892748: Epoch   0 Batch 2620/3125   train_loss = 1.107\n",
      "2019-05-03T19:54:49.174581: Epoch   0 Batch 2640/3125   train_loss = 1.134\n",
      "2019-05-03T19:54:51.436386: Epoch   0 Batch 2660/3125   train_loss = 1.233\n",
      "2019-05-03T19:54:53.707136: Epoch   0 Batch 2680/3125   train_loss = 1.038\n",
      "2019-05-03T19:54:55.999408: Epoch   0 Batch 2700/3125   train_loss = 1.175\n",
      "2019-05-03T19:54:58.289569: Epoch   0 Batch 2720/3125   train_loss = 1.192\n",
      "2019-05-03T19:55:00.594172: Epoch   0 Batch 2740/3125   train_loss = 1.154\n",
      "2019-05-03T19:55:02.886793: Epoch   0 Batch 2760/3125   train_loss = 1.149\n",
      "2019-05-03T19:55:05.168478: Epoch   0 Batch 2780/3125   train_loss = 1.090\n",
      "2019-05-03T19:55:07.440342: Epoch   0 Batch 2800/3125   train_loss = 1.471\n",
      "2019-05-03T19:55:09.763060: Epoch   0 Batch 2820/3125   train_loss = 1.374\n",
      "2019-05-03T19:55:12.054207: Epoch   0 Batch 2840/3125   train_loss = 1.136\n",
      "2019-05-03T19:55:14.357980: Epoch   0 Batch 2860/3125   train_loss = 1.356\n",
      "2019-05-03T19:55:16.678982: Epoch   0 Batch 2880/3125   train_loss = 1.127\n",
      "2019-05-03T19:55:19.001856: Epoch   0 Batch 2900/3125   train_loss = 1.176\n",
      "2019-05-03T19:55:21.276253: Epoch   0 Batch 2920/3125   train_loss = 1.284\n",
      "2019-05-03T19:55:23.577755: Epoch   0 Batch 2940/3125   train_loss = 1.133\n",
      "2019-05-03T19:55:25.859297: Epoch   0 Batch 2960/3125   train_loss = 1.291\n",
      "2019-05-03T19:55:28.141767: Epoch   0 Batch 2980/3125   train_loss = 1.156\n",
      "2019-05-03T19:55:30.414730: Epoch   0 Batch 3000/3125   train_loss = 1.180\n",
      "2019-05-03T19:55:32.738201: Epoch   0 Batch 3020/3125   train_loss = 1.269\n",
      "2019-05-03T19:55:35.048245: Epoch   0 Batch 3040/3125   train_loss = 1.241\n",
      "2019-05-03T19:55:37.323115: Epoch   0 Batch 3060/3125   train_loss = 1.211\n",
      "2019-05-03T19:55:39.614663: Epoch   0 Batch 3080/3125   train_loss = 1.257\n",
      "2019-05-03T19:55:41.905864: Epoch   0 Batch 3100/3125   train_loss = 1.237\n",
      "2019-05-03T19:55:44.146397: Epoch   0 Batch 3120/3125   train_loss = 1.031\n",
      "2019-05-03T19:55:45.204326: Epoch   0 Batch    0/781   test_loss = 1.044\n",
      "2019-05-03T19:55:45.536449: Epoch   0 Batch   20/781   test_loss = 1.190\n",
      "2019-05-03T19:55:45.857956: Epoch   0 Batch   40/781   test_loss = 1.094\n",
      "2019-05-03T19:55:46.189951: Epoch   0 Batch   60/781   test_loss = 1.328\n",
      "2019-05-03T19:55:46.509908: Epoch   0 Batch   80/781   test_loss = 1.351\n",
      "2019-05-03T19:55:46.832500: Epoch   0 Batch  100/781   test_loss = 1.383\n",
      "2019-05-03T19:55:47.154645: Epoch   0 Batch  120/781   test_loss = 1.290\n",
      "2019-05-03T19:55:47.475962: Epoch   0 Batch  140/781   test_loss = 1.182\n",
      "2019-05-03T19:55:47.804842: Epoch   0 Batch  160/781   test_loss = 1.399\n",
      "2019-05-03T19:55:48.129134: Epoch   0 Batch  180/781   test_loss = 1.220\n",
      "2019-05-03T19:55:48.484873: Epoch   0 Batch  200/781   test_loss = 1.183\n",
      "2019-05-03T19:55:48.832298: Epoch   0 Batch  220/781   test_loss = 1.024\n",
      "2019-05-03T19:55:49.176663: Epoch   0 Batch  240/781   test_loss = 1.204\n",
      "2019-05-03T19:55:49.507824: Epoch   0 Batch  260/781   test_loss = 1.181\n",
      "2019-05-03T19:55:49.839906: Epoch   0 Batch  280/781   test_loss = 1.472\n",
      "2019-05-03T19:55:50.170346: Epoch   0 Batch  300/781   test_loss = 1.224\n",
      "2019-05-03T19:55:50.483672: Epoch   0 Batch  320/781   test_loss = 1.330\n",
      "2019-05-03T19:55:50.805558: Epoch   0 Batch  340/781   test_loss = 0.843\n",
      "2019-05-03T19:55:51.137522: Epoch   0 Batch  360/781   test_loss = 1.308\n",
      "2019-05-03T19:55:51.458725: Epoch   0 Batch  380/781   test_loss = 1.142\n",
      "2019-05-03T19:55:51.779930: Epoch   0 Batch  400/781   test_loss = 1.083\n",
      "2019-05-03T19:55:52.101864: Epoch   0 Batch  420/781   test_loss = 1.111\n",
      "2019-05-03T19:55:52.423141: Epoch   0 Batch  440/781   test_loss = 1.297\n",
      "2019-05-03T19:55:52.744638: Epoch   0 Batch  460/781   test_loss = 1.106\n",
      "2019-05-03T19:55:53.076836: Epoch   0 Batch  480/781   test_loss = 1.168\n",
      "2019-05-03T19:55:53.428703: Epoch   0 Batch  500/781   test_loss = 0.963\n",
      "2019-05-03T19:55:53.809029: Epoch   0 Batch  520/781   test_loss = 1.179\n",
      "2019-05-03T19:55:54.182949: Epoch   0 Batch  540/781   test_loss = 1.014\n",
      "2019-05-03T19:55:54.594719: Epoch   0 Batch  560/781   test_loss = 1.376\n",
      "2019-05-03T19:55:54.957632: Epoch   0 Batch  580/781   test_loss = 1.229\n",
      "2019-05-03T19:55:55.298881: Epoch   0 Batch  600/781   test_loss = 1.133\n",
      "2019-05-03T19:55:55.640255: Epoch   0 Batch  620/781   test_loss = 1.300\n",
      "2019-05-03T19:55:55.991848: Epoch   0 Batch  640/781   test_loss = 1.297\n",
      "2019-05-03T19:55:56.333177: Epoch   0 Batch  660/781   test_loss = 1.131\n",
      "2019-05-03T19:55:56.655332: Epoch   0 Batch  680/781   test_loss = 1.410\n",
      "2019-05-03T19:55:56.986841: Epoch   0 Batch  700/781   test_loss = 1.211\n",
      "2019-05-03T19:55:57.308707: Epoch   0 Batch  720/781   test_loss = 1.336\n",
      "2019-05-03T19:55:57.632824: Epoch   0 Batch  740/781   test_loss = 1.251\n",
      "2019-05-03T19:55:57.951057: Epoch   0 Batch  760/781   test_loss = 1.207\n",
      "2019-05-03T19:55:58.290893: Epoch   0 Batch  780/781   test_loss = 1.241\n",
      "2019-05-03T19:56:01.189325: Epoch   1 Batch   15/3125   train_loss = 1.315\n",
      "2019-05-03T19:56:03.442385: Epoch   1 Batch   35/3125   train_loss = 1.174\n",
      "2019-05-03T19:56:05.791305: Epoch   1 Batch   55/3125   train_loss = 1.297\n",
      "2019-05-03T19:56:08.073497: Epoch   1 Batch   75/3125   train_loss = 1.145\n",
      "2019-05-03T19:56:10.375620: Epoch   1 Batch   95/3125   train_loss = 0.996\n",
      "2019-05-03T19:56:12.651051: Epoch   1 Batch  115/3125   train_loss = 1.244\n",
      "2019-05-03T19:56:14.949648: Epoch   1 Batch  135/3125   train_loss = 1.051\n",
      "2019-05-03T19:56:17.229905: Epoch   1 Batch  155/3125   train_loss = 1.139\n",
      "2019-05-03T19:56:19.512794: Epoch   1 Batch  175/3125   train_loss = 1.109\n",
      "2019-05-03T19:56:21.854042: Epoch   1 Batch  195/3125   train_loss = 1.287\n",
      "2019-05-03T19:56:24.156406: Epoch   1 Batch  215/3125   train_loss = 1.162\n",
      "2019-05-03T19:56:26.460616: Epoch   1 Batch  235/3125   train_loss = 1.125\n",
      "2019-05-03T19:56:28.736541: Epoch   1 Batch  255/3125   train_loss = 1.335\n",
      "2019-05-03T19:56:31.007814: Epoch   1 Batch  275/3125   train_loss = 1.077\n",
      "2019-05-03T19:56:33.298566: Epoch   1 Batch  295/3125   train_loss = 1.103\n",
      "2019-05-03T19:56:35.577857: Epoch   1 Batch  315/3125   train_loss = 1.139\n",
      "2019-05-03T19:56:37.896640: Epoch   1 Batch  335/3125   train_loss = 0.952\n",
      "2019-05-03T19:56:40.162888: Epoch   1 Batch  355/3125   train_loss = 1.149\n",
      "2019-05-03T19:56:42.434541: Epoch   1 Batch  375/3125   train_loss = 1.267\n",
      "2019-05-03T19:56:44.722928: Epoch   1 Batch  395/3125   train_loss = 1.038\n",
      "2019-05-03T19:56:47.010137: Epoch   1 Batch  415/3125   train_loss = 1.323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-03T19:56:49.297057: Epoch   1 Batch  435/3125   train_loss = 1.217\n",
      "2019-05-03T19:56:51.712909: Epoch   1 Batch  455/3125   train_loss = 1.171\n",
      "2019-05-03T19:56:54.054813: Epoch   1 Batch  475/3125   train_loss = 1.138\n",
      "2019-05-03T19:56:56.339530: Epoch   1 Batch  495/3125   train_loss = 1.078\n",
      "2019-05-03T19:56:58.631151: Epoch   1 Batch  515/3125   train_loss = 1.228\n",
      "2019-05-03T19:57:00.948137: Epoch   1 Batch  535/3125   train_loss = 1.156\n",
      "2019-05-03T19:57:03.222370: Epoch   1 Batch  555/3125   train_loss = 1.230\n",
      "2019-05-03T19:57:05.533115: Epoch   1 Batch  575/3125   train_loss = 1.121\n",
      "2019-05-03T19:57:07.832931: Epoch   1 Batch  595/3125   train_loss = 1.293\n",
      "2019-05-03T19:57:10.162888: Epoch   1 Batch  615/3125   train_loss = 1.046\n",
      "2019-05-03T19:57:12.466091: Epoch   1 Batch  635/3125   train_loss = 1.199\n",
      "2019-05-03T19:57:14.797212: Epoch   1 Batch  655/3125   train_loss = 1.039\n",
      "2019-05-03T19:57:17.120622: Epoch   1 Batch  675/3125   train_loss = 0.979\n",
      "2019-05-03T19:57:19.441416: Epoch   1 Batch  695/3125   train_loss = 1.062\n",
      "2019-05-03T19:57:21.713470: Epoch   1 Batch  715/3125   train_loss = 1.137\n",
      "2019-05-03T19:57:23.997404: Epoch   1 Batch  735/3125   train_loss = 0.954\n",
      "2019-05-03T19:57:26.439731: Epoch   1 Batch  755/3125   train_loss = 1.254\n",
      "2019-05-03T19:57:28.949012: Epoch   1 Batch  775/3125   train_loss = 0.994\n",
      "2019-05-03T19:57:31.247102: Epoch   1 Batch  795/3125   train_loss = 1.236\n",
      "2019-05-03T19:57:33.538868: Epoch   1 Batch  815/3125   train_loss = 1.131\n",
      "2019-05-03T19:57:35.861796: Epoch   1 Batch  835/3125   train_loss = 1.156\n",
      "2019-05-03T19:57:38.184708: Epoch   1 Batch  855/3125   train_loss = 1.256\n",
      "2019-05-03T19:57:40.507183: Epoch   1 Batch  875/3125   train_loss = 1.200\n",
      "2019-05-03T19:57:42.831736: Epoch   1 Batch  895/3125   train_loss = 1.108\n",
      "2019-05-03T19:57:45.152341: Epoch   1 Batch  915/3125   train_loss = 1.110\n",
      "2019-05-03T19:57:47.465906: Epoch   1 Batch  935/3125   train_loss = 1.243\n",
      "2019-05-03T19:57:49.757299: Epoch   1 Batch  955/3125   train_loss = 1.226\n",
      "2019-05-03T19:57:52.050280: Epoch   1 Batch  975/3125   train_loss = 1.071\n",
      "2019-05-03T19:57:54.340687: Epoch   1 Batch  995/3125   train_loss = 0.878\n",
      "2019-05-03T19:57:56.637233: Epoch   1 Batch 1015/3125   train_loss = 1.064\n",
      "2019-05-03T19:57:58.966982: Epoch   1 Batch 1035/3125   train_loss = 1.077\n",
      "2019-05-03T19:58:01.508517: Epoch   1 Batch 1055/3125   train_loss = 1.108\n",
      "2019-05-03T19:58:03.777513: Epoch   1 Batch 1075/3125   train_loss = 1.106\n",
      "2019-05-03T19:58:06.089433: Epoch   1 Batch 1095/3125   train_loss = 1.022\n",
      "2019-05-03T19:58:08.430666: Epoch   1 Batch 1115/3125   train_loss = 1.138\n",
      "2019-05-03T19:58:10.742244: Epoch   1 Batch 1135/3125   train_loss = 1.071\n",
      "2019-05-03T19:58:13.067277: Epoch   1 Batch 1155/3125   train_loss = 1.101\n",
      "2019-05-03T19:58:15.412782: Epoch   1 Batch 1175/3125   train_loss = 1.168\n",
      "2019-05-03T19:58:17.725996: Epoch   1 Batch 1195/3125   train_loss = 1.238\n",
      "2019-05-03T19:58:20.059671: Epoch   1 Batch 1215/3125   train_loss = 0.959\n",
      "2019-05-03T19:58:22.354933: Epoch   1 Batch 1235/3125   train_loss = 1.131\n",
      "2019-05-03T19:58:24.668888: Epoch   1 Batch 1255/3125   train_loss = 1.028\n",
      "2019-05-03T19:58:26.917312: Epoch   1 Batch 1275/3125   train_loss = 1.031\n",
      "2019-05-03T19:58:29.244272: Epoch   1 Batch 1295/3125   train_loss = 1.094\n",
      "2019-05-03T19:58:31.566549: Epoch   1 Batch 1315/3125   train_loss = 1.193\n",
      "2019-05-03T19:58:33.838881: Epoch   1 Batch 1335/3125   train_loss = 1.065\n",
      "2019-05-03T19:58:36.119325: Epoch   1 Batch 1355/3125   train_loss = 1.070\n",
      "2019-05-03T19:58:38.442525: Epoch   1 Batch 1375/3125   train_loss = 1.209\n",
      "2019-05-03T19:58:40.765063: Epoch   1 Batch 1395/3125   train_loss = 1.063\n",
      "2019-05-03T19:58:43.051331: Epoch   1 Batch 1415/3125   train_loss = 1.159\n",
      "2019-05-03T19:58:45.398013: Epoch   1 Batch 1435/3125   train_loss = 1.202\n",
      "2019-05-03T19:58:47.729133: Epoch   1 Batch 1455/3125   train_loss = 1.249\n",
      "2019-05-03T19:58:50.184306: Epoch   1 Batch 1475/3125   train_loss = 1.179\n",
      "2019-05-03T19:58:52.468558: Epoch   1 Batch 1495/3125   train_loss = 1.133\n",
      "2019-05-03T19:58:54.752611: Epoch   1 Batch 1515/3125   train_loss = 0.924\n",
      "2019-05-03T19:58:57.074110: Epoch   1 Batch 1535/3125   train_loss = 0.900\n",
      "2019-05-03T19:58:59.351571: Epoch   1 Batch 1555/3125   train_loss = 1.103\n",
      "2019-05-03T19:59:01.705411: Epoch   1 Batch 1575/3125   train_loss = 1.064\n",
      "2019-05-03T19:59:03.965952: Epoch   1 Batch 1595/3125   train_loss = 1.113\n",
      "2019-05-03T19:59:06.299836: Epoch   1 Batch 1615/3125   train_loss = 0.995\n",
      "2019-05-03T19:59:08.591259: Epoch   1 Batch 1635/3125   train_loss = 1.076\n",
      "2019-05-03T19:59:10.883602: Epoch   1 Batch 1655/3125   train_loss = 1.196\n",
      "2019-05-03T19:59:13.192373: Epoch   1 Batch 1675/3125   train_loss = 1.046\n",
      "2019-05-03T19:59:15.517220: Epoch   1 Batch 1695/3125   train_loss = 1.023\n",
      "2019-05-03T19:59:17.825007: Epoch   1 Batch 1715/3125   train_loss = 0.976\n",
      "2019-05-03T19:59:20.142076: Epoch   1 Batch 1735/3125   train_loss = 1.225\n",
      "2019-05-03T19:59:22.432341: Epoch   1 Batch 1755/3125   train_loss = 1.042\n",
      "2019-05-03T19:59:24.732276: Epoch   1 Batch 1775/3125   train_loss = 1.070\n",
      "2019-05-03T19:59:27.012559: Epoch   1 Batch 1795/3125   train_loss = 1.109\n",
      "2019-05-03T19:59:29.310961: Epoch   1 Batch 1815/3125   train_loss = 1.051\n",
      "2019-05-03T19:59:31.593521: Epoch   1 Batch 1835/3125   train_loss = 1.125\n",
      "2019-05-03T19:59:33.917835: Epoch   1 Batch 1855/3125   train_loss = 1.029\n",
      "2019-05-03T19:59:36.247143: Epoch   1 Batch 1875/3125   train_loss = 1.152\n",
      "2019-05-03T19:59:38.537924: Epoch   1 Batch 1895/3125   train_loss = 0.979\n",
      "2019-05-03T19:59:40.983535: Epoch   1 Batch 1915/3125   train_loss = 1.027\n",
      "2019-05-03T19:59:43.252857: Epoch   1 Batch 1935/3125   train_loss = 1.044\n",
      "2019-05-03T19:59:45.560088: Epoch   1 Batch 1955/3125   train_loss = 0.972\n",
      "2019-05-03T19:59:47.850885: Epoch   1 Batch 1975/3125   train_loss = 1.022\n",
      "2019-05-03T19:59:50.169239: Epoch   1 Batch 1995/3125   train_loss = 1.279\n",
      "2019-05-03T19:59:52.462986: Epoch   1 Batch 2015/3125   train_loss = 1.185\n",
      "2019-05-03T19:59:54.743036: Epoch   1 Batch 2035/3125   train_loss = 1.129\n",
      "2019-05-03T19:59:57.058867: Epoch   1 Batch 2055/3125   train_loss = 1.058\n",
      "2019-05-03T19:59:59.468201: Epoch   1 Batch 2075/3125   train_loss = 1.167\n",
      "2019-05-03T20:00:01.791116: Epoch   1 Batch 2095/3125   train_loss = 1.097\n",
      "2019-05-03T20:00:04.125186: Epoch   1 Batch 2115/3125   train_loss = 1.156\n",
      "2019-05-03T20:00:06.454937: Epoch   1 Batch 2135/3125   train_loss = 1.059\n",
      "2019-05-03T20:00:08.802932: Epoch   1 Batch 2155/3125   train_loss = 1.066\n",
      "2019-05-03T20:00:11.130882: Epoch   1 Batch 2175/3125   train_loss = 1.097\n",
      "2019-05-03T20:00:13.424961: Epoch   1 Batch 2195/3125   train_loss = 1.084\n",
      "2019-05-03T20:00:15.739327: Epoch   1 Batch 2215/3125   train_loss = 1.028\n",
      "2019-05-03T20:00:18.056417: Epoch   1 Batch 2235/3125   train_loss = 1.199\n",
      "2019-05-03T20:00:20.374819: Epoch   1 Batch 2255/3125   train_loss = 1.175\n",
      "2019-05-03T20:00:22.720083: Epoch   1 Batch 2275/3125   train_loss = 0.900\n",
      "2019-05-03T20:00:24.997964: Epoch   1 Batch 2295/3125   train_loss = 1.265\n",
      "2019-05-03T20:00:27.307552: Epoch   1 Batch 2315/3125   train_loss = 1.134\n",
      "2019-05-03T20:00:29.575575: Epoch   1 Batch 2335/3125   train_loss = 1.110\n",
      "2019-05-03T20:00:31.903214: Epoch   1 Batch 2355/3125   train_loss = 1.121\n",
      "2019-05-03T20:00:34.181939: Epoch   1 Batch 2375/3125   train_loss = 1.256\n",
      "2019-05-03T20:00:36.503466: Epoch   1 Batch 2395/3125   train_loss = 1.046\n",
      "2019-05-03T20:00:38.768227: Epoch   1 Batch 2415/3125   train_loss = 1.132\n",
      "2019-05-03T20:00:41.070867: Epoch   1 Batch 2435/3125   train_loss = 1.003\n",
      "2019-05-03T20:00:43.394572: Epoch   1 Batch 2455/3125   train_loss = 1.112\n",
      "2019-05-03T20:00:45.712470: Epoch   1 Batch 2475/3125   train_loss = 0.966\n",
      "2019-05-03T20:00:48.124259: Epoch   1 Batch 2495/3125   train_loss = 1.043\n",
      "2019-05-03T20:00:50.421902: Epoch   1 Batch 2515/3125   train_loss = 1.055\n",
      "2019-05-03T20:00:52.722011: Epoch   1 Batch 2535/3125   train_loss = 1.082\n",
      "2019-05-03T20:00:55.012729: Epoch   1 Batch 2555/3125   train_loss = 0.957\n",
      "2019-05-03T20:00:57.293451: Epoch   1 Batch 2575/3125   train_loss = 0.958\n",
      "2019-05-03T20:00:59.586220: Epoch   1 Batch 2595/3125   train_loss = 1.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-03T20:01:01.870582: Epoch   1 Batch 2615/3125   train_loss = 1.162\n",
      "2019-05-03T20:01:04.169512: Epoch   1 Batch 2635/3125   train_loss = 1.001\n",
      "2019-05-03T20:01:06.449217: Epoch   1 Batch 2655/3125   train_loss = 1.035\n",
      "2019-05-03T20:01:08.755522: Epoch   1 Batch 2675/3125   train_loss = 0.971\n",
      "2019-05-03T20:01:11.047356: Epoch   1 Batch 2695/3125   train_loss = 1.108\n",
      "2019-05-03T20:01:13.362754: Epoch   1 Batch 2715/3125   train_loss = 1.015\n",
      "2019-05-03T20:01:15.780109: Epoch   1 Batch 2735/3125   train_loss = 0.902\n",
      "2019-05-03T20:01:18.113815: Epoch   1 Batch 2755/3125   train_loss = 1.123\n",
      "2019-05-03T20:01:20.413101: Epoch   1 Batch 2775/3125   train_loss = 1.158\n",
      "2019-05-03T20:01:22.737890: Epoch   1 Batch 2795/3125   train_loss = 1.054\n",
      "2019-05-03T20:01:25.003374: Epoch   1 Batch 2815/3125   train_loss = 0.998\n",
      "2019-05-03T20:01:27.323705: Epoch   1 Batch 2835/3125   train_loss = 1.060\n",
      "2019-05-03T20:01:29.628098: Epoch   1 Batch 2855/3125   train_loss = 1.098\n",
      "2019-05-03T20:01:31.916925: Epoch   1 Batch 2875/3125   train_loss = 0.994\n",
      "2019-05-03T20:01:34.215614: Epoch   1 Batch 2895/3125   train_loss = 1.099\n",
      "2019-05-03T20:01:36.526627: Epoch   1 Batch 2915/3125   train_loss = 1.012\n",
      "2019-05-03T20:01:38.801062: Epoch   1 Batch 2935/3125   train_loss = 1.165\n",
      "2019-05-03T20:01:41.112104: Epoch   1 Batch 2955/3125   train_loss = 1.148\n",
      "2019-05-03T20:01:43.409760: Epoch   1 Batch 2975/3125   train_loss = 1.011\n",
      "2019-05-03T20:01:45.715452: Epoch   1 Batch 2995/3125   train_loss = 0.921\n",
      "2019-05-03T20:01:48.003909: Epoch   1 Batch 3015/3125   train_loss = 0.946\n",
      "2019-05-03T20:01:50.292832: Epoch   1 Batch 3035/3125   train_loss = 1.070\n",
      "2019-05-03T20:01:52.604087: Epoch   1 Batch 3055/3125   train_loss = 1.091\n",
      "2019-05-03T20:01:54.911321: Epoch   1 Batch 3075/3125   train_loss = 1.001\n",
      "2019-05-03T20:01:57.214834: Epoch   1 Batch 3095/3125   train_loss = 1.015\n",
      "2019-05-03T20:02:00.184835: Epoch   1 Batch 3115/3125   train_loss = 0.877\n",
      "2019-05-03T20:02:01.580266: Epoch   1 Batch   19/781   test_loss = 1.048\n",
      "2019-05-03T20:02:01.916855: Epoch   1 Batch   39/781   test_loss = 0.898\n",
      "2019-05-03T20:02:02.245125: Epoch   1 Batch   59/781   test_loss = 0.979\n",
      "2019-05-03T20:02:02.578920: Epoch   1 Batch   79/781   test_loss = 1.058\n",
      "2019-05-03T20:02:02.902127: Epoch   1 Batch   99/781   test_loss = 1.052\n",
      "2019-05-03T20:02:03.234069: Epoch   1 Batch  119/781   test_loss = 0.971\n",
      "2019-05-03T20:02:03.545296: Epoch   1 Batch  139/781   test_loss = 1.052\n",
      "2019-05-03T20:02:03.885814: Epoch   1 Batch  159/781   test_loss = 1.065\n",
      "2019-05-03T20:02:04.196604: Epoch   1 Batch  179/781   test_loss = 0.951\n",
      "2019-05-03T20:02:04.529119: Epoch   1 Batch  199/781   test_loss = 0.925\n",
      "2019-05-03T20:02:04.856284: Epoch   1 Batch  219/781   test_loss = 1.070\n",
      "2019-05-03T20:02:05.196348: Epoch   1 Batch  239/781   test_loss = 1.168\n",
      "2019-05-03T20:02:05.527721: Epoch   1 Batch  259/781   test_loss = 1.023\n",
      "2019-05-03T20:02:05.842412: Epoch   1 Batch  279/781   test_loss = 1.134\n",
      "2019-05-03T20:02:06.163409: Epoch   1 Batch  299/781   test_loss = 1.225\n",
      "2019-05-03T20:02:06.500182: Epoch   1 Batch  319/781   test_loss = 1.055\n",
      "2019-05-03T20:02:06.822236: Epoch   1 Batch  339/781   test_loss = 0.891\n",
      "2019-05-03T20:02:07.145879: Epoch   1 Batch  359/781   test_loss = 0.921\n",
      "2019-05-03T20:02:07.464607: Epoch   1 Batch  379/781   test_loss = 1.042\n",
      "2019-05-03T20:02:07.809192: Epoch   1 Batch  399/781   test_loss = 0.956\n",
      "2019-05-03T20:02:08.142382: Epoch   1 Batch  419/781   test_loss = 1.025\n",
      "2019-05-03T20:02:08.464396: Epoch   1 Batch  439/781   test_loss = 1.014\n",
      "2019-05-03T20:02:08.789482: Epoch   1 Batch  459/781   test_loss = 1.088\n",
      "2019-05-03T20:02:09.113889: Epoch   1 Batch  479/781   test_loss = 1.105\n",
      "2019-05-03T20:02:09.432342: Epoch   1 Batch  499/781   test_loss = 0.938\n",
      "2019-05-03T20:02:09.758632: Epoch   1 Batch  519/781   test_loss = 1.087\n",
      "2019-05-03T20:02:10.103035: Epoch   1 Batch  539/781   test_loss = 0.918\n",
      "2019-05-03T20:02:10.422970: Epoch   1 Batch  559/781   test_loss = 1.155\n",
      "2019-05-03T20:02:10.756848: Epoch   1 Batch  579/781   test_loss = 1.044\n",
      "2019-05-03T20:02:11.065386: Epoch   1 Batch  599/781   test_loss = 1.031\n",
      "2019-05-03T20:02:11.399633: Epoch   1 Batch  619/781   test_loss = 1.201\n",
      "2019-05-03T20:02:11.731313: Epoch   1 Batch  639/781   test_loss = 0.936\n",
      "2019-05-03T20:02:12.049455: Epoch   1 Batch  659/781   test_loss = 1.074\n",
      "2019-05-03T20:02:12.365515: Epoch   1 Batch  679/781   test_loss = 1.207\n",
      "2019-05-03T20:02:12.706172: Epoch   1 Batch  699/781   test_loss = 0.864\n",
      "2019-05-03T20:02:13.018011: Epoch   1 Batch  719/781   test_loss = 1.026\n",
      "2019-05-03T20:02:13.338475: Epoch   1 Batch  739/781   test_loss = 0.985\n",
      "2019-05-03T20:02:13.682009: Epoch   1 Batch  759/781   test_loss = 0.941\n",
      "2019-05-03T20:02:13.999518: Epoch   1 Batch  779/781   test_loss = 0.842\n",
      "2019-05-03T20:02:15.980723: Epoch   2 Batch   10/3125   train_loss = 1.012\n",
      "2019-05-03T20:02:18.316471: Epoch   2 Batch   30/3125   train_loss = 1.045\n",
      "2019-05-03T20:02:20.645412: Epoch   2 Batch   50/3125   train_loss = 1.087\n",
      "2019-05-03T20:02:22.987157: Epoch   2 Batch   70/3125   train_loss = 1.075\n",
      "2019-05-03T20:02:25.276882: Epoch   2 Batch   90/3125   train_loss = 1.029\n",
      "2019-05-03T20:02:27.579403: Epoch   2 Batch  110/3125   train_loss = 0.993\n",
      "2019-05-03T20:02:29.909117: Epoch   2 Batch  130/3125   train_loss = 1.042\n",
      "2019-05-03T20:02:32.212001: Epoch   2 Batch  150/3125   train_loss = 1.160\n",
      "2019-05-03T20:02:34.490122: Epoch   2 Batch  170/3125   train_loss = 1.036\n",
      "2019-05-03T20:02:36.886054: Epoch   2 Batch  190/3125   train_loss = 1.084\n",
      "2019-05-03T20:02:39.184426: Epoch   2 Batch  210/3125   train_loss = 0.992\n",
      "2019-05-03T20:02:41.500667: Epoch   2 Batch  230/3125   train_loss = 1.016\n",
      "2019-05-03T20:02:43.780028: Epoch   2 Batch  250/3125   train_loss = 0.942\n",
      "2019-05-03T20:02:46.101729: Epoch   2 Batch  270/3125   train_loss = 0.875\n",
      "2019-05-03T20:02:48.427018: Epoch   2 Batch  290/3125   train_loss = 1.062\n",
      "2019-05-03T20:02:50.727395: Epoch   2 Batch  310/3125   train_loss = 1.050\n",
      "2019-05-03T20:02:53.018840: Epoch   2 Batch  330/3125   train_loss = 1.087\n",
      "2019-05-03T20:02:55.320645: Epoch   2 Batch  350/3125   train_loss = 0.951\n",
      "2019-05-03T20:02:57.646611: Epoch   2 Batch  370/3125   train_loss = 1.253\n",
      "2019-05-03T20:03:00.093199: Epoch   2 Batch  390/3125   train_loss = 1.132\n",
      "2019-05-03T20:03:02.465199: Epoch   2 Batch  410/3125   train_loss = 0.951\n",
      "2019-05-03T20:03:04.776183: Epoch   2 Batch  430/3125   train_loss = 1.158\n",
      "2019-05-03T20:03:07.168059: Epoch   2 Batch  450/3125   train_loss = 0.985\n",
      "2019-05-03T20:03:09.468362: Epoch   2 Batch  470/3125   train_loss = 1.011\n",
      "2019-05-03T20:03:11.762565: Epoch   2 Batch  490/3125   train_loss = 1.025\n",
      "2019-05-03T20:03:14.041922: Epoch   2 Batch  510/3125   train_loss = 1.120\n",
      "2019-05-03T20:03:16.361268: Epoch   2 Batch  530/3125   train_loss = 1.021\n",
      "2019-05-03T20:03:18.659308: Epoch   2 Batch  550/3125   train_loss = 0.977\n",
      "2019-05-03T20:03:20.962449: Epoch   2 Batch  570/3125   train_loss = 1.040\n",
      "2019-05-03T20:03:23.361407: Epoch   2 Batch  590/3125   train_loss = 1.057\n",
      "2019-05-03T20:03:25.671832: Epoch   2 Batch  610/3125   train_loss = 0.994\n",
      "2019-05-03T20:03:27.943379: Epoch   2 Batch  630/3125   train_loss = 0.985\n",
      "2019-05-03T20:03:30.215871: Epoch   2 Batch  650/3125   train_loss = 1.049\n",
      "2019-05-03T20:03:32.526340: Epoch   2 Batch  670/3125   train_loss = 1.019\n",
      "2019-05-03T20:03:34.824932: Epoch   2 Batch  690/3125   train_loss = 0.996\n",
      "2019-05-03T20:03:37.151028: Epoch   2 Batch  710/3125   train_loss = 0.935\n",
      "2019-05-03T20:03:39.457781: Epoch   2 Batch  730/3125   train_loss = 0.823\n",
      "2019-05-03T20:03:41.762302: Epoch   2 Batch  750/3125   train_loss = 0.965\n",
      "2019-05-03T20:03:44.027887: Epoch   2 Batch  770/3125   train_loss = 0.961\n",
      "2019-05-03T20:03:46.341729: Epoch   2 Batch  790/3125   train_loss = 0.908\n",
      "2019-05-03T20:03:48.636082: Epoch   2 Batch  810/3125   train_loss = 0.869\n",
      "2019-05-03T20:03:50.962429: Epoch   2 Batch  830/3125   train_loss = 0.871\n",
      "2019-05-03T20:03:53.290249: Epoch   2 Batch  850/3125   train_loss = 1.057\n",
      "2019-05-03T20:03:55.634203: Epoch   2 Batch  870/3125   train_loss = 0.900\n",
      "2019-05-03T20:03:57.913403: Epoch   2 Batch  890/3125   train_loss = 0.997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-03T20:04:00.295062: Epoch   2 Batch  910/3125   train_loss = 1.025\n",
      "2019-05-03T20:04:02.622287: Epoch   2 Batch  930/3125   train_loss = 1.047\n",
      "2019-05-03T20:04:04.912551: Epoch   2 Batch  950/3125   train_loss = 0.935\n",
      "2019-05-03T20:04:07.246198: Epoch   2 Batch  970/3125   train_loss = 1.118\n",
      "2019-05-03T20:04:09.588002: Epoch   2 Batch  990/3125   train_loss = 0.841\n",
      "2019-05-03T20:04:11.941903: Epoch   2 Batch 1010/3125   train_loss = 1.118\n",
      "2019-05-03T20:04:14.220367: Epoch   2 Batch 1030/3125   train_loss = 0.875\n",
      "2019-05-03T20:04:16.542678: Epoch   2 Batch 1050/3125   train_loss = 0.950\n",
      "2019-05-03T20:04:18.832990: Epoch   2 Batch 1070/3125   train_loss = 1.065\n",
      "2019-05-03T20:04:21.134312: Epoch   2 Batch 1090/3125   train_loss = 1.111\n",
      "2019-05-03T20:04:23.456742: Epoch   2 Batch 1110/3125   train_loss = 1.165\n",
      "2019-05-03T20:04:25.823759: Epoch   2 Batch 1130/3125   train_loss = 0.982\n",
      "2019-05-03T20:04:28.226333: Epoch   2 Batch 1150/3125   train_loss = 0.951\n",
      "2019-05-03T20:04:30.524738: Epoch   2 Batch 1170/3125   train_loss = 1.004\n",
      "2019-05-03T20:04:32.824097: Epoch   2 Batch 1190/3125   train_loss = 1.051\n",
      "2019-05-03T20:04:35.077185: Epoch   2 Batch 1210/3125   train_loss = 0.903\n",
      "2019-05-03T20:04:37.439965: Epoch   2 Batch 1230/3125   train_loss = 0.922\n",
      "2019-05-03T20:04:39.721016: Epoch   2 Batch 1250/3125   train_loss = 1.001\n",
      "2019-05-03T20:04:42.073603: Epoch   2 Batch 1270/3125   train_loss = 0.998\n",
      "2019-05-03T20:04:44.391839: Epoch   2 Batch 1290/3125   train_loss = 0.946\n",
      "2019-05-03T20:04:46.714004: Epoch   2 Batch 1310/3125   train_loss = 1.091\n",
      "2019-05-03T20:04:49.025136: Epoch   2 Batch 1330/3125   train_loss = 1.105\n",
      "2019-05-03T20:04:51.349002: Epoch   2 Batch 1350/3125   train_loss = 0.940\n",
      "2019-05-03T20:04:53.650519: Epoch   2 Batch 1370/3125   train_loss = 0.897\n",
      "2019-05-03T20:04:55.987136: Epoch   2 Batch 1390/3125   train_loss = 1.097\n",
      "2019-05-03T20:04:58.556375: Epoch   2 Batch 1410/3125   train_loss = 1.080\n",
      "2019-05-03T20:05:00.865407: Epoch   2 Batch 1430/3125   train_loss = 1.028\n",
      "2019-05-03T20:05:03.207108: Epoch   2 Batch 1450/3125   train_loss = 0.998\n",
      "2019-05-03T20:05:05.560298: Epoch   2 Batch 1470/3125   train_loss = 1.038\n",
      "2019-05-03T20:05:07.883703: Epoch   2 Batch 1490/3125   train_loss = 1.043\n",
      "2019-05-03T20:05:10.222088: Epoch   2 Batch 1510/3125   train_loss = 0.974\n",
      "2019-05-03T20:05:12.511553: Epoch   2 Batch 1530/3125   train_loss = 1.110\n",
      "2019-05-03T20:05:14.841617: Epoch   2 Batch 1550/3125   train_loss = 0.799\n",
      "2019-05-03T20:05:17.119587: Epoch   2 Batch 1570/3125   train_loss = 0.940\n",
      "2019-05-03T20:05:19.440980: Epoch   2 Batch 1590/3125   train_loss = 1.033\n",
      "2019-05-03T20:05:21.759999: Epoch   2 Batch 1610/3125   train_loss = 1.053\n",
      "2019-05-03T20:05:24.074548: Epoch   2 Batch 1630/3125   train_loss = 1.097\n",
      "2019-05-03T20:05:26.406280: Epoch   2 Batch 1650/3125   train_loss = 0.872\n",
      "2019-05-03T20:05:28.735530: Epoch   2 Batch 1670/3125   train_loss = 0.839\n",
      "2019-05-03T20:05:31.064178: Epoch   2 Batch 1690/3125   train_loss = 1.018\n",
      "2019-05-03T20:05:33.392217: Epoch   2 Batch 1710/3125   train_loss = 1.015\n",
      "2019-05-03T20:05:35.671030: Epoch   2 Batch 1730/3125   train_loss = 0.974\n",
      "2019-05-03T20:05:38.004690: Epoch   2 Batch 1750/3125   train_loss = 0.880\n",
      "2019-05-03T20:05:40.308267: Epoch   2 Batch 1770/3125   train_loss = 1.127\n",
      "2019-05-03T20:05:42.621028: Epoch   2 Batch 1790/3125   train_loss = 0.985\n",
      "2019-05-03T20:05:44.928225: Epoch   2 Batch 1810/3125   train_loss = 0.965\n",
      "2019-05-03T20:05:47.307347: Epoch   2 Batch 1830/3125   train_loss = 1.005\n",
      "2019-05-03T20:05:49.604045: Epoch   2 Batch 1850/3125   train_loss = 0.980\n",
      "2019-05-03T20:05:51.963566: Epoch   2 Batch 1870/3125   train_loss = 1.015\n",
      "2019-05-03T20:05:54.338061: Epoch   2 Batch 1890/3125   train_loss = 0.855\n",
      "2019-05-03T20:05:56.761864: Epoch   2 Batch 1910/3125   train_loss = 0.947\n",
      "2019-05-03T20:05:59.247842: Epoch   2 Batch 1930/3125   train_loss = 0.998\n",
      "2019-05-03T20:06:02.276820: Epoch   2 Batch 1950/3125   train_loss = 0.887\n",
      "2019-05-03T20:06:04.634434: Epoch   2 Batch 1970/3125   train_loss = 0.996\n",
      "2019-05-03T20:06:07.035606: Epoch   2 Batch 1990/3125   train_loss = 0.842\n",
      "2019-05-03T20:06:09.457937: Epoch   2 Batch 2010/3125   train_loss = 0.832\n",
      "2019-05-03T20:06:11.765282: Epoch   2 Batch 2030/3125   train_loss = 0.929\n",
      "2019-05-03T20:06:14.062614: Epoch   2 Batch 2050/3125   train_loss = 0.995\n",
      "2019-05-03T20:06:16.382646: Epoch   2 Batch 2070/3125   train_loss = 0.911\n",
      "2019-05-03T20:06:18.733486: Epoch   2 Batch 2090/3125   train_loss = 0.932\n",
      "2019-05-03T20:06:21.057894: Epoch   2 Batch 2110/3125   train_loss = 1.083\n",
      "2019-05-03T20:06:23.381010: Epoch   2 Batch 2130/3125   train_loss = 0.916\n",
      "2019-05-03T20:06:25.706466: Epoch   2 Batch 2150/3125   train_loss = 1.007\n",
      "2019-05-03T20:06:28.073745: Epoch   2 Batch 2170/3125   train_loss = 0.907\n",
      "2019-05-03T20:06:30.418697: Epoch   2 Batch 2190/3125   train_loss = 0.936\n",
      "2019-05-03T20:06:32.747556: Epoch   2 Batch 2210/3125   train_loss = 0.965\n",
      "2019-05-03T20:06:35.069567: Epoch   2 Batch 2230/3125   train_loss = 0.928\n",
      "2019-05-03T20:06:37.400314: Epoch   2 Batch 2250/3125   train_loss = 0.991\n",
      "2019-05-03T20:06:39.723971: Epoch   2 Batch 2270/3125   train_loss = 0.931\n",
      "2019-05-03T20:06:42.073126: Epoch   2 Batch 2290/3125   train_loss = 0.885\n",
      "2019-05-03T20:06:44.396426: Epoch   2 Batch 2310/3125   train_loss = 0.904\n",
      "2019-05-03T20:06:46.712233: Epoch   2 Batch 2330/3125   train_loss = 1.063\n",
      "2019-05-03T20:06:49.021041: Epoch   2 Batch 2350/3125   train_loss = 1.024\n",
      "2019-05-03T20:06:51.355738: Epoch   2 Batch 2370/3125   train_loss = 0.946\n",
      "2019-05-03T20:06:53.654113: Epoch   2 Batch 2390/3125   train_loss = 1.001\n",
      "2019-05-03T20:06:55.961478: Epoch   2 Batch 2410/3125   train_loss = 1.094\n",
      "2019-05-03T20:06:58.419335: Epoch   2 Batch 2430/3125   train_loss = 0.932\n",
      "2019-05-03T20:07:00.727850: Epoch   2 Batch 2450/3125   train_loss = 1.003\n",
      "2019-05-03T20:07:03.045042: Epoch   2 Batch 2470/3125   train_loss = 1.062\n",
      "2019-05-03T20:07:05.387377: Epoch   2 Batch 2490/3125   train_loss = 1.061\n",
      "2019-05-03T20:07:07.728448: Epoch   2 Batch 2510/3125   train_loss = 1.102\n",
      "2019-05-03T20:07:10.039455: Epoch   2 Batch 2530/3125   train_loss = 0.801\n",
      "2019-05-03T20:07:12.353396: Epoch   2 Batch 2550/3125   train_loss = 1.040\n",
      "2019-05-03T20:07:14.669449: Epoch   2 Batch 2570/3125   train_loss = 0.999\n",
      "2019-05-03T20:07:16.964904: Epoch   2 Batch 2590/3125   train_loss = 0.978\n",
      "2019-05-03T20:07:19.317620: Epoch   2 Batch 2610/3125   train_loss = 0.979\n",
      "2019-05-03T20:07:21.659680: Epoch   2 Batch 2630/3125   train_loss = 0.699\n",
      "2019-05-03T20:07:23.983592: Epoch   2 Batch 2650/3125   train_loss = 0.981\n",
      "2019-05-03T20:07:26.288650: Epoch   2 Batch 2670/3125   train_loss = 0.995\n",
      "2019-05-03T20:07:28.655250: Epoch   2 Batch 2690/3125   train_loss = 0.958\n",
      "2019-05-03T20:07:30.974212: Epoch   2 Batch 2710/3125   train_loss = 0.914\n",
      "2019-05-03T20:07:33.279726: Epoch   2 Batch 2730/3125   train_loss = 1.147\n",
      "2019-05-03T20:07:35.596659: Epoch   2 Batch 2750/3125   train_loss = 1.045\n",
      "2019-05-03T20:07:37.959183: Epoch   2 Batch 2770/3125   train_loss = 0.957\n",
      "2019-05-03T20:07:40.282853: Epoch   2 Batch 2790/3125   train_loss = 0.896\n",
      "2019-05-03T20:07:42.597651: Epoch   2 Batch 2810/3125   train_loss = 0.998\n",
      "2019-05-03T20:07:44.898594: Epoch   2 Batch 2830/3125   train_loss = 0.851\n",
      "2019-05-03T20:07:47.182807: Epoch   2 Batch 2850/3125   train_loss = 0.989\n",
      "2019-05-03T20:07:49.521595: Epoch   2 Batch 2870/3125   train_loss = 0.844\n",
      "2019-05-03T20:07:51.846905: Epoch   2 Batch 2890/3125   train_loss = 0.836\n",
      "2019-05-03T20:07:54.179743: Epoch   2 Batch 2910/3125   train_loss = 1.004\n",
      "2019-05-03T20:07:56.488207: Epoch   2 Batch 2930/3125   train_loss = 0.832\n",
      "2019-05-03T20:07:58.803474: Epoch   2 Batch 2950/3125   train_loss = 1.060\n",
      "2019-05-03T20:08:01.806949: Epoch   2 Batch 2970/3125   train_loss = 0.943\n",
      "2019-05-03T20:08:04.114154: Epoch   2 Batch 2990/3125   train_loss = 0.919\n",
      "2019-05-03T20:08:06.408056: Epoch   2 Batch 3010/3125   train_loss = 0.974\n",
      "2019-05-03T20:08:08.752000: Epoch   2 Batch 3030/3125   train_loss = 0.970\n",
      "2019-05-03T20:08:11.133154: Epoch   2 Batch 3050/3125   train_loss = 0.905\n",
      "2019-05-03T20:08:13.437798: Epoch   2 Batch 3070/3125   train_loss = 0.858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-03T20:08:15.743421: Epoch   2 Batch 3090/3125   train_loss = 0.865\n",
      "2019-05-03T20:08:18.056628: Epoch   2 Batch 3110/3125   train_loss = 0.851\n",
      "2019-05-03T20:08:20.009915: Epoch   2 Batch   18/781   test_loss = 0.802\n",
      "2019-05-03T20:08:20.322301: Epoch   2 Batch   38/781   test_loss = 0.894\n",
      "2019-05-03T20:08:20.660195: Epoch   2 Batch   58/781   test_loss = 0.858\n",
      "2019-05-03T20:08:20.972724: Epoch   2 Batch   78/781   test_loss = 0.906\n",
      "2019-05-03T20:08:21.305337: Epoch   2 Batch   98/781   test_loss = 0.927\n",
      "2019-05-03T20:08:21.643796: Epoch   2 Batch  118/781   test_loss = 0.846\n",
      "2019-05-03T20:08:21.963291: Epoch   2 Batch  138/781   test_loss = 1.017\n",
      "2019-05-03T20:08:22.289235: Epoch   2 Batch  158/781   test_loss = 0.869\n",
      "2019-05-03T20:08:22.621094: Epoch   2 Batch  178/781   test_loss = 0.797\n",
      "2019-05-03T20:08:22.948256: Epoch   2 Batch  198/781   test_loss = 0.884\n",
      "2019-05-03T20:08:23.273816: Epoch   2 Batch  218/781   test_loss = 1.043\n",
      "2019-05-03T20:08:23.594263: Epoch   2 Batch  238/781   test_loss = 0.979\n",
      "2019-05-03T20:08:23.916401: Epoch   2 Batch  258/781   test_loss = 1.024\n",
      "2019-05-03T20:08:24.240588: Epoch   2 Batch  278/781   test_loss = 1.046\n",
      "2019-05-03T20:08:24.559327: Epoch   2 Batch  298/781   test_loss = 0.867\n",
      "2019-05-03T20:08:24.886161: Epoch   2 Batch  318/781   test_loss = 0.918\n",
      "2019-05-03T20:08:25.222356: Epoch   2 Batch  338/781   test_loss = 1.001\n",
      "2019-05-03T20:08:25.574220: Epoch   2 Batch  358/781   test_loss = 0.947\n",
      "2019-05-03T20:08:25.902065: Epoch   2 Batch  378/781   test_loss = 0.863\n",
      "2019-05-03T20:08:26.241382: Epoch   2 Batch  398/781   test_loss = 0.820\n",
      "2019-05-03T20:08:26.559344: Epoch   2 Batch  418/781   test_loss = 0.957\n",
      "2019-05-03T20:08:26.887758: Epoch   2 Batch  438/781   test_loss = 1.017\n",
      "2019-05-03T20:08:27.208615: Epoch   2 Batch  458/781   test_loss = 0.920\n",
      "2019-05-03T20:08:27.542636: Epoch   2 Batch  478/781   test_loss = 0.941\n",
      "2019-05-03T20:08:27.870226: Epoch   2 Batch  498/781   test_loss = 0.798\n",
      "2019-05-03T20:08:28.181605: Epoch   2 Batch  518/781   test_loss = 0.940\n",
      "2019-05-03T20:08:28.507213: Epoch   2 Batch  538/781   test_loss = 0.792\n",
      "2019-05-03T20:08:28.836025: Epoch   2 Batch  558/781   test_loss = 0.821\n",
      "2019-05-03T20:08:29.175192: Epoch   2 Batch  578/781   test_loss = 0.902\n",
      "2019-05-03T20:08:29.511791: Epoch   2 Batch  598/781   test_loss = 1.095\n",
      "2019-05-03T20:08:29.842412: Epoch   2 Batch  618/781   test_loss = 0.892\n",
      "2019-05-03T20:08:30.178205: Epoch   2 Batch  638/781   test_loss = 0.930\n",
      "2019-05-03T20:08:30.518073: Epoch   2 Batch  658/781   test_loss = 1.054\n",
      "2019-05-03T20:08:30.830024: Epoch   2 Batch  678/781   test_loss = 0.952\n",
      "2019-05-03T20:08:31.159202: Epoch   2 Batch  698/781   test_loss = 0.892\n",
      "2019-05-03T20:08:31.509258: Epoch   2 Batch  718/781   test_loss = 1.009\n",
      "2019-05-03T20:08:31.839221: Epoch   2 Batch  738/781   test_loss = 0.838\n",
      "2019-05-03T20:08:32.159766: Epoch   2 Batch  758/781   test_loss = 1.015\n",
      "2019-05-03T20:08:32.477826: Epoch   2 Batch  778/781   test_loss = 0.901\n",
      "2019-05-03T20:08:33.892057: Epoch   3 Batch    5/3125   train_loss = 0.911\n",
      "2019-05-03T20:08:36.214714: Epoch   3 Batch   25/3125   train_loss = 1.014\n",
      "2019-05-03T20:08:38.541108: Epoch   3 Batch   45/3125   train_loss = 0.815\n",
      "2019-05-03T20:08:40.848409: Epoch   3 Batch   65/3125   train_loss = 1.007\n",
      "2019-05-03T20:08:43.228119: Epoch   3 Batch   85/3125   train_loss = 0.846\n",
      "2019-05-03T20:08:45.558854: Epoch   3 Batch  105/3125   train_loss = 0.749\n",
      "2019-05-03T20:08:47.877004: Epoch   3 Batch  125/3125   train_loss = 0.909\n",
      "2019-05-03T20:08:50.186186: Epoch   3 Batch  145/3125   train_loss = 0.922\n",
      "2019-05-03T20:08:52.510187: Epoch   3 Batch  165/3125   train_loss = 0.948\n",
      "2019-05-03T20:08:54.827317: Epoch   3 Batch  185/3125   train_loss = 0.859\n",
      "2019-05-03T20:08:57.136324: Epoch   3 Batch  205/3125   train_loss = 0.826\n",
      "2019-05-03T20:08:59.500813: Epoch   3 Batch  225/3125   train_loss = 0.827\n",
      "2019-05-03T20:09:01.823702: Epoch   3 Batch  245/3125   train_loss = 1.093\n",
      "2019-05-03T20:09:04.132109: Epoch   3 Batch  265/3125   train_loss = 0.924\n",
      "2019-05-03T20:09:06.438075: Epoch   3 Batch  285/3125   train_loss = 0.974\n",
      "2019-05-03T20:09:08.757401: Epoch   3 Batch  305/3125   train_loss = 0.854\n",
      "2019-05-03T20:09:11.201522: Epoch   3 Batch  325/3125   train_loss = 0.932\n",
      "2019-05-03T20:09:13.511711: Epoch   3 Batch  345/3125   train_loss = 0.947\n",
      "2019-05-03T20:09:15.808315: Epoch   3 Batch  365/3125   train_loss = 0.846\n",
      "2019-05-03T20:09:18.121268: Epoch   3 Batch  385/3125   train_loss = 0.875\n",
      "2019-05-03T20:09:20.428330: Epoch   3 Batch  405/3125   train_loss = 0.887\n",
      "2019-05-03T20:09:22.752545: Epoch   3 Batch  425/3125   train_loss = 0.990\n",
      "2019-05-03T20:09:25.060566: Epoch   3 Batch  445/3125   train_loss = 0.974\n",
      "2019-05-03T20:09:27.388404: Epoch   3 Batch  465/3125   train_loss = 0.904\n",
      "2019-05-03T20:09:29.732828: Epoch   3 Batch  485/3125   train_loss = 1.026\n",
      "2019-05-03T20:09:32.119677: Epoch   3 Batch  505/3125   train_loss = 0.832\n",
      "2019-05-03T20:09:34.441044: Epoch   3 Batch  525/3125   train_loss = 0.987\n",
      "2019-05-03T20:09:36.760500: Epoch   3 Batch  545/3125   train_loss = 0.879\n",
      "2019-05-03T20:09:39.100213: Epoch   3 Batch  565/3125   train_loss = 1.137\n",
      "2019-05-03T20:09:41.398559: Epoch   3 Batch  585/3125   train_loss = 0.911\n",
      "2019-05-03T20:09:43.721909: Epoch   3 Batch  605/3125   train_loss = 0.881\n",
      "2019-05-03T20:09:46.135827: Epoch   3 Batch  625/3125   train_loss = 0.960\n",
      "2019-05-03T20:09:48.496586: Epoch   3 Batch  645/3125   train_loss = 0.949\n",
      "2019-05-03T20:09:50.841642: Epoch   3 Batch  665/3125   train_loss = 1.040\n",
      "2019-05-03T20:09:53.152646: Epoch   3 Batch  685/3125   train_loss = 0.941\n",
      "2019-05-03T20:09:55.494741: Epoch   3 Batch  705/3125   train_loss = 1.038\n",
      "2019-05-03T20:09:57.896973: Epoch   3 Batch  725/3125   train_loss = 0.883\n",
      "2019-05-03T20:10:00.583044: Epoch   3 Batch  745/3125   train_loss = 0.918\n",
      "2019-05-03T20:10:03.043988: Epoch   3 Batch  765/3125   train_loss = 0.894\n",
      "2019-05-03T20:10:05.418692: Epoch   3 Batch  785/3125   train_loss = 1.075\n",
      "2019-05-03T20:10:07.717080: Epoch   3 Batch  805/3125   train_loss = 0.848\n",
      "2019-05-03T20:10:10.073055: Epoch   3 Batch  825/3125   train_loss = 0.923\n",
      "2019-05-03T20:10:12.417442: Epoch   3 Batch  845/3125   train_loss = 0.972\n",
      "2019-05-03T20:10:14.733768: Epoch   3 Batch  865/3125   train_loss = 0.998\n",
      "2019-05-03T20:10:17.059706: Epoch   3 Batch  885/3125   train_loss = 0.944\n",
      "2019-05-03T20:10:19.394788: Epoch   3 Batch  905/3125   train_loss = 1.094\n",
      "2019-05-03T20:10:21.727947: Epoch   3 Batch  925/3125   train_loss = 0.910\n",
      "2019-05-03T20:10:24.039464: Epoch   3 Batch  945/3125   train_loss = 0.945\n",
      "2019-05-03T20:10:26.357345: Epoch   3 Batch  965/3125   train_loss = 0.846\n",
      "2019-05-03T20:10:28.678135: Epoch   3 Batch  985/3125   train_loss = 1.021\n",
      "2019-05-03T20:10:31.010691: Epoch   3 Batch 1005/3125   train_loss = 0.818\n",
      "2019-05-03T20:10:33.419498: Epoch   3 Batch 1025/3125   train_loss = 0.883\n",
      "2019-05-03T20:10:35.751967: Epoch   3 Batch 1045/3125   train_loss = 1.084\n",
      "2019-05-03T20:10:38.107064: Epoch   3 Batch 1065/3125   train_loss = 0.890\n",
      "2019-05-03T20:10:40.384297: Epoch   3 Batch 1085/3125   train_loss = 0.825\n",
      "2019-05-03T20:10:42.693081: Epoch   3 Batch 1105/3125   train_loss = 0.860\n",
      "2019-05-03T20:10:45.015099: Epoch   3 Batch 1125/3125   train_loss = 0.847\n",
      "2019-05-03T20:10:47.323220: Epoch   3 Batch 1145/3125   train_loss = 0.904\n",
      "2019-05-03T20:10:49.651285: Epoch   3 Batch 1165/3125   train_loss = 0.980\n",
      "2019-05-03T20:10:51.981626: Epoch   3 Batch 1185/3125   train_loss = 0.874\n",
      "2019-05-03T20:10:54.289775: Epoch   3 Batch 1205/3125   train_loss = 0.894\n",
      "2019-05-03T20:10:56.604564: Epoch   3 Batch 1225/3125   train_loss = 1.000\n",
      "2019-05-03T20:10:58.907909: Epoch   3 Batch 1245/3125   train_loss = 1.087\n",
      "2019-05-03T20:11:01.222369: Epoch   3 Batch 1265/3125   train_loss = 1.004\n",
      "2019-05-03T20:11:03.555827: Epoch   3 Batch 1285/3125   train_loss = 1.056\n",
      "2019-05-03T20:11:05.886959: Epoch   3 Batch 1305/3125   train_loss = 0.806\n",
      "2019-05-03T20:11:08.183287: Epoch   3 Batch 1325/3125   train_loss = 0.827\n",
      "2019-05-03T20:11:10.665353: Epoch   3 Batch 1345/3125   train_loss = 0.910\n",
      "2019-05-03T20:11:12.997979: Epoch   3 Batch 1365/3125   train_loss = 0.773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-03T20:11:15.317477: Epoch   3 Batch 1385/3125   train_loss = 0.837\n",
      "2019-05-03T20:11:17.643320: Epoch   3 Batch 1405/3125   train_loss = 0.851\n",
      "2019-05-03T20:11:19.945898: Epoch   3 Batch 1425/3125   train_loss = 1.063\n",
      "2019-05-03T20:11:22.275181: Epoch   3 Batch 1445/3125   train_loss = 1.049\n",
      "2019-05-03T20:11:24.593394: Epoch   3 Batch 1465/3125   train_loss = 0.929\n",
      "2019-05-03T20:11:26.918738: Epoch   3 Batch 1485/3125   train_loss = 0.929\n",
      "2019-05-03T20:11:29.251215: Epoch   3 Batch 1505/3125   train_loss = 0.774\n",
      "2019-05-03T20:11:31.563907: Epoch   3 Batch 1525/3125   train_loss = 0.822\n",
      "2019-05-03T20:11:33.877638: Epoch   3 Batch 1545/3125   train_loss = 0.928\n",
      "2019-05-03T20:11:36.201911: Epoch   3 Batch 1565/3125   train_loss = 0.981\n",
      "2019-05-03T20:11:38.561970: Epoch   3 Batch 1585/3125   train_loss = 0.831\n",
      "2019-05-03T20:11:40.869027: Epoch   3 Batch 1605/3125   train_loss = 0.933\n",
      "2019-05-03T20:11:43.212896: Epoch   3 Batch 1625/3125   train_loss = 0.990\n",
      "2019-05-03T20:11:45.523147: Epoch   3 Batch 1645/3125   train_loss = 0.950\n",
      "2019-05-03T20:11:47.849583: Epoch   3 Batch 1665/3125   train_loss = 0.918\n",
      "2019-05-03T20:11:50.160142: Epoch   3 Batch 1685/3125   train_loss = 0.996\n",
      "2019-05-03T20:11:52.712613: Epoch   3 Batch 1705/3125   train_loss = 0.906\n",
      "2019-05-03T20:11:55.196034: Epoch   3 Batch 1725/3125   train_loss = 0.855\n",
      "2019-05-03T20:11:57.529242: Epoch   3 Batch 1745/3125   train_loss = 0.798\n",
      "2019-05-03T20:12:00.379852: Epoch   3 Batch 1765/3125   train_loss = 0.865\n",
      "2019-05-03T20:12:02.700250: Epoch   3 Batch 1785/3125   train_loss = 1.042\n",
      "2019-05-03T20:12:05.011245: Epoch   3 Batch 1805/3125   train_loss = 0.961\n",
      "2019-05-03T20:12:07.322268: Epoch   3 Batch 1825/3125   train_loss = 1.076\n",
      "2019-05-03T20:12:09.668705: Epoch   3 Batch 1845/3125   train_loss = 0.960\n",
      "2019-05-03T20:12:12.073577: Epoch   3 Batch 1865/3125   train_loss = 0.811\n",
      "2019-05-03T20:12:14.394344: Epoch   3 Batch 1885/3125   train_loss = 0.953\n",
      "2019-05-03T20:12:16.685224: Epoch   3 Batch 1905/3125   train_loss = 0.829\n",
      "2019-05-03T20:12:19.006400: Epoch   3 Batch 1925/3125   train_loss = 0.856\n",
      "2019-05-03T20:12:21.348516: Epoch   3 Batch 1945/3125   train_loss = 0.933\n",
      "2019-05-03T20:12:23.702336: Epoch   3 Batch 1965/3125   train_loss = 0.807\n",
      "2019-05-03T20:12:26.051324: Epoch   3 Batch 1985/3125   train_loss = 0.836\n",
      "2019-05-03T20:12:28.392918: Epoch   3 Batch 2005/3125   train_loss = 0.922\n",
      "2019-05-03T20:12:30.713936: Epoch   3 Batch 2025/3125   train_loss = 0.956\n",
      "2019-05-03T20:12:33.034773: Epoch   3 Batch 2045/3125   train_loss = 0.788\n",
      "2019-05-03T20:12:35.369066: Epoch   3 Batch 2065/3125   train_loss = 0.739\n",
      "2019-05-03T20:12:37.716725: Epoch   3 Batch 2085/3125   train_loss = 1.038\n",
      "2019-05-03T20:12:40.023965: Epoch   3 Batch 2105/3125   train_loss = 0.869\n",
      "2019-05-03T20:12:42.402026: Epoch   3 Batch 2125/3125   train_loss = 0.903\n",
      "2019-05-03T20:12:44.729889: Epoch   3 Batch 2145/3125   train_loss = 1.005\n",
      "2019-05-03T20:12:47.056517: Epoch   3 Batch 2165/3125   train_loss = 0.855\n",
      "2019-05-03T20:12:49.393745: Epoch   3 Batch 2185/3125   train_loss = 0.942\n",
      "2019-05-03T20:12:51.728439: Epoch   3 Batch 2205/3125   train_loss = 0.961\n",
      "2019-05-03T20:12:54.037597: Epoch   3 Batch 2225/3125   train_loss = 0.863\n",
      "2019-05-03T20:12:56.343912: Epoch   3 Batch 2245/3125   train_loss = 0.821\n",
      "2019-05-03T20:12:58.667620: Epoch   3 Batch 2265/3125   train_loss = 0.954\n",
      "2019-05-03T20:13:00.981583: Epoch   3 Batch 2285/3125   train_loss = 1.056\n",
      "2019-05-03T20:13:03.292085: Epoch   3 Batch 2305/3125   train_loss = 0.846\n",
      "2019-05-03T20:13:05.603648: Epoch   3 Batch 2325/3125   train_loss = 0.803\n",
      "2019-05-03T20:13:07.910915: Epoch   3 Batch 2345/3125   train_loss = 0.884\n",
      "2019-05-03T20:13:10.377550: Epoch   3 Batch 2365/3125   train_loss = 0.722\n",
      "2019-05-03T20:13:12.699023: Epoch   3 Batch 2385/3125   train_loss = 0.942\n",
      "2019-05-03T20:13:15.051346: Epoch   3 Batch 2405/3125   train_loss = 0.901\n",
      "2019-05-03T20:13:17.378860: Epoch   3 Batch 2425/3125   train_loss = 0.805\n",
      "2019-05-03T20:13:19.667587: Epoch   3 Batch 2445/3125   train_loss = 0.939\n",
      "2019-05-03T20:13:21.993598: Epoch   3 Batch 2465/3125   train_loss = 0.818\n",
      "2019-05-03T20:13:24.285030: Epoch   3 Batch 2485/3125   train_loss = 0.851\n",
      "2019-05-03T20:13:26.592524: Epoch   3 Batch 2505/3125   train_loss = 0.874\n",
      "2019-05-03T20:13:28.909555: Epoch   3 Batch 2525/3125   train_loss = 0.883\n",
      "2019-05-03T20:13:31.242768: Epoch   3 Batch 2545/3125   train_loss = 1.016\n",
      "2019-05-03T20:13:33.582028: Epoch   3 Batch 2565/3125   train_loss = 0.825\n",
      "2019-05-03T20:13:35.872059: Epoch   3 Batch 2585/3125   train_loss = 0.800\n",
      "2019-05-03T20:13:38.214062: Epoch   3 Batch 2605/3125   train_loss = 0.797\n",
      "2019-05-03T20:13:40.538613: Epoch   3 Batch 2625/3125   train_loss = 1.010\n",
      "2019-05-03T20:13:42.871896: Epoch   3 Batch 2645/3125   train_loss = 0.892\n",
      "2019-05-03T20:13:45.204828: Epoch   3 Batch 2665/3125   train_loss = 0.929\n",
      "2019-05-03T20:13:47.567537: Epoch   3 Batch 2685/3125   train_loss = 0.950\n",
      "2019-05-03T20:13:49.880336: Epoch   3 Batch 2705/3125   train_loss = 0.833\n",
      "2019-05-03T20:13:52.202796: Epoch   3 Batch 2725/3125   train_loss = 0.992\n",
      "2019-05-03T20:13:54.505567: Epoch   3 Batch 2745/3125   train_loss = 0.942\n",
      "2019-05-03T20:13:56.816881: Epoch   3 Batch 2765/3125   train_loss = 0.816\n",
      "2019-05-03T20:13:59.139012: Epoch   3 Batch 2785/3125   train_loss = 0.940\n",
      "2019-05-03T20:14:01.531144: Epoch   3 Batch 2805/3125   train_loss = 0.838\n",
      "2019-05-03T20:14:03.846145: Epoch   3 Batch 2825/3125   train_loss = 0.897\n",
      "2019-05-03T20:14:06.148429: Epoch   3 Batch 2845/3125   train_loss = 0.894\n",
      "2019-05-03T20:14:08.490836: Epoch   3 Batch 2865/3125   train_loss = 0.872\n",
      "2019-05-03T20:14:10.812668: Epoch   3 Batch 2885/3125   train_loss = 0.969\n",
      "2019-05-03T20:14:13.115436: Epoch   3 Batch 2905/3125   train_loss = 0.943\n",
      "2019-05-03T20:14:15.427532: Epoch   3 Batch 2925/3125   train_loss = 0.911\n",
      "2019-05-03T20:14:17.770581: Epoch   3 Batch 2945/3125   train_loss = 0.954\n",
      "2019-05-03T20:14:20.132404: Epoch   3 Batch 2965/3125   train_loss = 0.999\n",
      "2019-05-03T20:14:22.442615: Epoch   3 Batch 2985/3125   train_loss = 0.834\n",
      "2019-05-03T20:14:24.737603: Epoch   3 Batch 3005/3125   train_loss = 0.873\n",
      "2019-05-03T20:14:27.027513: Epoch   3 Batch 3025/3125   train_loss = 0.922\n",
      "2019-05-03T20:14:29.183437: Epoch   3 Batch 3045/3125   train_loss = 0.913\n",
      "2019-05-03T20:14:31.184457: Epoch   3 Batch 3065/3125   train_loss = 0.839\n",
      "2019-05-03T20:14:33.457784: Epoch   3 Batch 3085/3125   train_loss = 0.925\n",
      "2019-05-03T20:14:35.832320: Epoch   3 Batch 3105/3125   train_loss = 0.920\n",
      "2019-05-03T20:14:38.307051: Epoch   3 Batch   17/781   test_loss = 0.865\n",
      "2019-05-03T20:14:38.658650: Epoch   3 Batch   37/781   test_loss = 0.914\n",
      "2019-05-03T20:14:38.990227: Epoch   3 Batch   57/781   test_loss = 0.996\n",
      "2019-05-03T20:14:39.339018: Epoch   3 Batch   77/781   test_loss = 0.886\n",
      "2019-05-03T20:14:39.663087: Epoch   3 Batch   97/781   test_loss = 0.798\n",
      "2019-05-03T20:14:39.992493: Epoch   3 Batch  117/781   test_loss = 0.917\n",
      "2019-05-03T20:14:40.314091: Epoch   3 Batch  137/781   test_loss = 0.927\n",
      "2019-05-03T20:14:40.647484: Epoch   3 Batch  157/781   test_loss = 0.920\n",
      "2019-05-03T20:14:40.977138: Epoch   3 Batch  177/781   test_loss = 0.943\n",
      "2019-05-03T20:14:41.300601: Epoch   3 Batch  197/781   test_loss = 0.940\n",
      "2019-05-03T20:14:41.652015: Epoch   3 Batch  217/781   test_loss = 0.737\n",
      "2019-05-03T20:14:41.974010: Epoch   3 Batch  237/781   test_loss = 0.758\n",
      "2019-05-03T20:14:42.295533: Epoch   3 Batch  257/781   test_loss = 1.036\n",
      "2019-05-03T20:14:42.626654: Epoch   3 Batch  277/781   test_loss = 0.984\n",
      "2019-05-03T20:14:42.947835: Epoch   3 Batch  297/781   test_loss = 0.959\n",
      "2019-05-03T20:14:43.279814: Epoch   3 Batch  317/781   test_loss = 1.045\n",
      "2019-05-03T20:14:43.619647: Epoch   3 Batch  337/781   test_loss = 0.949\n",
      "2019-05-03T20:14:43.943354: Epoch   3 Batch  357/781   test_loss = 0.918\n",
      "2019-05-03T20:14:44.265500: Epoch   3 Batch  377/781   test_loss = 0.919\n",
      "2019-05-03T20:14:44.604514: Epoch   3 Batch  397/781   test_loss = 0.910\n",
      "2019-05-03T20:14:44.928008: Epoch   3 Batch  417/781   test_loss = 0.859\n",
      "2019-05-03T20:14:45.249686: Epoch   3 Batch  437/781   test_loss = 0.833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-03T20:14:45.581371: Epoch   3 Batch  457/781   test_loss = 0.726\n",
      "2019-05-03T20:14:45.902721: Epoch   3 Batch  477/781   test_loss = 0.919\n",
      "2019-05-03T20:14:46.224888: Epoch   3 Batch  497/781   test_loss = 0.889\n",
      "2019-05-03T20:14:46.547560: Epoch   3 Batch  517/781   test_loss = 0.826\n",
      "2019-05-03T20:14:46.877279: Epoch   3 Batch  537/781   test_loss = 0.845\n",
      "2019-05-03T20:14:47.198922: Epoch   3 Batch  557/781   test_loss = 1.021\n",
      "2019-05-03T20:14:47.533401: Epoch   3 Batch  577/781   test_loss = 0.930\n",
      "2019-05-03T20:14:47.854603: Epoch   3 Batch  597/781   test_loss = 0.852\n",
      "2019-05-03T20:14:48.176306: Epoch   3 Batch  617/781   test_loss = 0.901\n",
      "2019-05-03T20:14:48.498269: Epoch   3 Batch  637/781   test_loss = 0.816\n",
      "2019-05-03T20:14:48.819441: Epoch   3 Batch  657/781   test_loss = 1.047\n",
      "2019-05-03T20:14:49.159927: Epoch   3 Batch  677/781   test_loss = 0.990\n",
      "2019-05-03T20:14:49.494715: Epoch   3 Batch  697/781   test_loss = 0.899\n",
      "2019-05-03T20:14:49.836186: Epoch   3 Batch  717/781   test_loss = 0.846\n",
      "2019-05-03T20:14:50.166108: Epoch   3 Batch  737/781   test_loss = 0.768\n",
      "2019-05-03T20:14:50.480025: Epoch   3 Batch  757/781   test_loss = 1.027\n",
      "2019-05-03T20:14:50.811595: Epoch   3 Batch  777/781   test_loss = 0.893\n",
      "2019-05-03T20:14:51.774590: Epoch   4 Batch    0/3125   train_loss = 1.067\n",
      "2019-05-03T20:14:54.053349: Epoch   4 Batch   20/3125   train_loss = 0.843\n",
      "2019-05-03T20:14:56.350598: Epoch   4 Batch   40/3125   train_loss = 0.936\n",
      "2019-05-03T20:14:58.653861: Epoch   4 Batch   60/3125   train_loss = 0.722\n",
      "2019-05-03T20:15:00.944079: Epoch   4 Batch   80/3125   train_loss = 0.898\n",
      "2019-05-03T20:15:03.251509: Epoch   4 Batch  100/3125   train_loss = 0.938\n",
      "2019-05-03T20:15:05.562381: Epoch   4 Batch  120/3125   train_loss = 0.972\n",
      "2019-05-03T20:15:07.863447: Epoch   4 Batch  140/3125   train_loss = 0.943\n",
      "2019-05-03T20:15:10.190596: Epoch   4 Batch  160/3125   train_loss = 0.802\n",
      "2019-05-03T20:15:12.495856: Epoch   4 Batch  180/3125   train_loss = 0.904\n",
      "2019-05-03T20:15:14.826811: Epoch   4 Batch  200/3125   train_loss = 1.074\n",
      "2019-05-03T20:15:17.104355: Epoch   4 Batch  220/3125   train_loss = 0.877\n",
      "2019-05-03T20:15:19.427709: Epoch   4 Batch  240/3125   train_loss = 0.962\n",
      "2019-05-03T20:15:21.915423: Epoch   4 Batch  260/3125   train_loss = 0.976\n",
      "2019-05-03T20:15:24.219135: Epoch   4 Batch  280/3125   train_loss = 0.961\n",
      "2019-05-03T20:15:26.522913: Epoch   4 Batch  300/3125   train_loss = 1.056\n",
      "2019-05-03T20:15:28.849158: Epoch   4 Batch  320/3125   train_loss = 0.982\n",
      "2019-05-03T20:15:31.124102: Epoch   4 Batch  340/3125   train_loss = 0.806\n",
      "2019-05-03T20:15:33.466456: Epoch   4 Batch  360/3125   train_loss = 0.807\n",
      "2019-05-03T20:15:35.761837: Epoch   4 Batch  380/3125   train_loss = 0.893\n",
      "2019-05-03T20:15:38.131536: Epoch   4 Batch  400/3125   train_loss = 0.851\n",
      "2019-05-03T20:15:40.473463: Epoch   4 Batch  420/3125   train_loss = 0.820\n",
      "2019-05-03T20:15:42.801286: Epoch   4 Batch  440/3125   train_loss = 0.849\n",
      "2019-05-03T20:15:45.110169: Epoch   4 Batch  460/3125   train_loss = 0.937\n",
      "2019-05-03T20:15:47.400286: Epoch   4 Batch  480/3125   train_loss = 1.036\n",
      "2019-05-03T20:15:49.692846: Epoch   4 Batch  500/3125   train_loss = 0.692\n",
      "2019-05-03T20:15:51.962610: Epoch   4 Batch  520/3125   train_loss = 0.951\n",
      "2019-05-03T20:15:54.264382: Epoch   4 Batch  540/3125   train_loss = 0.804\n",
      "2019-05-03T20:15:56.566882: Epoch   4 Batch  560/3125   train_loss = 0.996\n",
      "2019-05-03T20:15:58.865459: Epoch   4 Batch  580/3125   train_loss = 0.953\n",
      "2019-05-03T20:16:01.275103: Epoch   4 Batch  600/3125   train_loss = 0.924\n",
      "2019-05-03T20:16:03.591895: Epoch   4 Batch  620/3125   train_loss = 0.909\n",
      "2019-05-03T20:16:05.892929: Epoch   4 Batch  640/3125   train_loss = 0.884\n",
      "2019-05-03T20:16:08.232420: Epoch   4 Batch  660/3125   train_loss = 0.917\n",
      "2019-05-03T20:16:10.608552: Epoch   4 Batch  680/3125   train_loss = 0.948\n",
      "2019-05-03T20:16:12.910363: Epoch   4 Batch  700/3125   train_loss = 0.959\n",
      "2019-05-03T20:16:15.214610: Epoch   4 Batch  720/3125   train_loss = 0.815\n",
      "2019-05-03T20:16:17.527398: Epoch   4 Batch  740/3125   train_loss = 0.909\n",
      "2019-05-03T20:16:19.825015: Epoch   4 Batch  760/3125   train_loss = 0.838\n",
      "2019-05-03T20:16:22.150219: Epoch   4 Batch  780/3125   train_loss = 0.935\n",
      "2019-05-03T20:16:24.444744: Epoch   4 Batch  800/3125   train_loss = 0.796\n",
      "2019-05-03T20:16:26.777751: Epoch   4 Batch  820/3125   train_loss = 0.830\n",
      "2019-05-03T20:16:29.090868: Epoch   4 Batch  840/3125   train_loss = 0.815\n",
      "2019-05-03T20:16:31.399859: Epoch   4 Batch  860/3125   train_loss = 0.820\n",
      "2019-05-03T20:16:33.700972: Epoch   4 Batch  880/3125   train_loss = 0.798\n",
      "2019-05-03T20:16:35.999671: Epoch   4 Batch  900/3125   train_loss = 0.882\n",
      "2019-05-03T20:16:38.326548: Epoch   4 Batch  920/3125   train_loss = 0.993\n",
      "2019-05-03T20:16:40.630083: Epoch   4 Batch  940/3125   train_loss = 0.867\n",
      "2019-05-03T20:16:43.035534: Epoch   4 Batch  960/3125   train_loss = 0.941\n",
      "2019-05-03T20:16:45.345117: Epoch   4 Batch  980/3125   train_loss = 0.945\n",
      "2019-05-03T20:16:47.666645: Epoch   4 Batch 1000/3125   train_loss = 0.921\n",
      "2019-05-03T20:16:49.967920: Epoch   4 Batch 1020/3125   train_loss = 0.957\n",
      "2019-05-03T20:16:52.261110: Epoch   4 Batch 1040/3125   train_loss = 0.857\n",
      "2019-05-03T20:16:54.555842: Epoch   4 Batch 1060/3125   train_loss = 0.972\n",
      "2019-05-03T20:16:56.839384: Epoch   4 Batch 1080/3125   train_loss = 0.849\n",
      "2019-05-03T20:16:59.172704: Epoch   4 Batch 1100/3125   train_loss = 0.873\n",
      "2019-05-03T20:17:01.479337: Epoch   4 Batch 1120/3125   train_loss = 0.881\n",
      "2019-05-03T20:17:03.802092: Epoch   4 Batch 1140/3125   train_loss = 0.879\n",
      "2019-05-03T20:17:06.111669: Epoch   4 Batch 1160/3125   train_loss = 0.821\n",
      "2019-05-03T20:17:08.442409: Epoch   4 Batch 1180/3125   train_loss = 0.883\n",
      "2019-05-03T20:17:10.771921: Epoch   4 Batch 1200/3125   train_loss = 0.994\n",
      "2019-05-03T20:17:13.074856: Epoch   4 Batch 1220/3125   train_loss = 1.007\n",
      "2019-05-03T20:17:15.414307: Epoch   4 Batch 1240/3125   train_loss = 0.754\n",
      "2019-05-03T20:17:17.754243: Epoch   4 Batch 1260/3125   train_loss = 0.931\n",
      "2019-05-03T20:17:20.207350: Epoch   4 Batch 1280/3125   train_loss = 0.903\n",
      "2019-05-03T20:17:22.524811: Epoch   4 Batch 1300/3125   train_loss = 0.862\n",
      "2019-05-03T20:17:24.824775: Epoch   4 Batch 1320/3125   train_loss = 0.893\n",
      "2019-05-03T20:17:27.152233: Epoch   4 Batch 1340/3125   train_loss = 0.754\n",
      "2019-05-03T20:17:29.486228: Epoch   4 Batch 1360/3125   train_loss = 0.823\n",
      "2019-05-03T20:17:31.808503: Epoch   4 Batch 1380/3125   train_loss = 0.836\n",
      "2019-05-03T20:17:34.108947: Epoch   4 Batch 1400/3125   train_loss = 0.952\n",
      "2019-05-03T20:17:36.421102: Epoch   4 Batch 1420/3125   train_loss = 0.862\n",
      "2019-05-03T20:17:38.738392: Epoch   4 Batch 1440/3125   train_loss = 0.775\n",
      "2019-05-03T20:17:41.054879: Epoch   4 Batch 1460/3125   train_loss = 0.901\n",
      "2019-05-03T20:17:43.385351: Epoch   4 Batch 1480/3125   train_loss = 0.815\n",
      "2019-05-03T20:17:45.734055: Epoch   4 Batch 1500/3125   train_loss = 0.894\n",
      "2019-05-03T20:17:48.066048: Epoch   4 Batch 1520/3125   train_loss = 0.798\n",
      "2019-05-03T20:17:50.392980: Epoch   4 Batch 1540/3125   train_loss = 0.995\n",
      "2019-05-03T20:17:52.985636: Epoch   4 Batch 1560/3125   train_loss = 0.865\n",
      "2019-05-03T20:17:55.294533: Epoch   4 Batch 1580/3125   train_loss = 0.994\n",
      "2019-05-03T20:17:57.605359: Epoch   4 Batch 1600/3125   train_loss = 0.827\n",
      "2019-05-03T20:18:00.015866: Epoch   4 Batch 1620/3125   train_loss = 0.825\n",
      "2019-05-03T20:18:02.353849: Epoch   4 Batch 1640/3125   train_loss = 0.949\n",
      "2019-05-03T20:18:04.680828: Epoch   4 Batch 1660/3125   train_loss = 0.976\n",
      "2019-05-03T20:18:06.962679: Epoch   4 Batch 1680/3125   train_loss = 0.912\n",
      "2019-05-03T20:18:09.286010: Epoch   4 Batch 1700/3125   train_loss = 0.791\n",
      "2019-05-03T20:18:11.631628: Epoch   4 Batch 1720/3125   train_loss = 0.887\n",
      "2019-05-03T20:18:13.939251: Epoch   4 Batch 1740/3125   train_loss = 0.955\n",
      "2019-05-03T20:18:16.241322: Epoch   4 Batch 1760/3125   train_loss = 0.892\n",
      "2019-05-03T20:18:18.604906: Epoch   4 Batch 1780/3125   train_loss = 0.913\n",
      "2019-05-03T20:18:20.879418: Epoch   4 Batch 1800/3125   train_loss = 0.826\n",
      "2019-05-03T20:18:23.194672: Epoch   4 Batch 1820/3125   train_loss = 0.845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-03T20:18:25.507992: Epoch   4 Batch 1840/3125   train_loss = 0.896\n",
      "2019-05-03T20:18:27.824060: Epoch   4 Batch 1860/3125   train_loss = 0.958\n",
      "2019-05-03T20:18:30.120829: Epoch   4 Batch 1880/3125   train_loss = 0.872\n",
      "2019-05-03T20:18:32.431905: Epoch   4 Batch 1900/3125   train_loss = 0.800\n",
      "2019-05-03T20:18:34.745013: Epoch   4 Batch 1920/3125   train_loss = 0.859\n",
      "2019-05-03T20:18:37.055451: Epoch   4 Batch 1940/3125   train_loss = 0.798\n",
      "2019-05-03T20:18:39.370644: Epoch   4 Batch 1960/3125   train_loss = 0.765\n",
      "2019-05-03T20:18:41.710705: Epoch   4 Batch 1980/3125   train_loss = 0.866\n",
      "2019-05-03T20:18:44.004607: Epoch   4 Batch 2000/3125   train_loss = 1.046\n",
      "2019-05-03T20:18:46.364127: Epoch   4 Batch 2020/3125   train_loss = 1.009\n",
      "2019-05-03T20:18:48.674349: Epoch   4 Batch 2040/3125   train_loss = 0.799\n",
      "2019-05-03T20:18:51.041746: Epoch   4 Batch 2060/3125   train_loss = 0.807\n",
      "2019-05-03T20:18:53.371238: Epoch   4 Batch 2080/3125   train_loss = 0.962\n",
      "2019-05-03T20:18:55.696887: Epoch   4 Batch 2100/3125   train_loss = 0.851\n",
      "2019-05-03T20:18:58.007294: Epoch   4 Batch 2120/3125   train_loss = 0.806\n",
      "2019-05-03T20:19:00.328538: Epoch   4 Batch 2140/3125   train_loss = 0.868\n",
      "2019-05-03T20:19:02.654525: Epoch   4 Batch 2160/3125   train_loss = 0.869\n",
      "2019-05-03T20:19:04.966032: Epoch   4 Batch 2180/3125   train_loss = 0.893\n",
      "2019-05-03T20:19:07.290141: Epoch   4 Batch 2200/3125   train_loss = 0.814\n",
      "2019-05-03T20:19:09.622820: Epoch   4 Batch 2220/3125   train_loss = 0.804\n",
      "2019-05-03T20:19:11.921436: Epoch   4 Batch 2240/3125   train_loss = 0.814\n",
      "2019-05-03T20:19:14.225277: Epoch   4 Batch 2260/3125   train_loss = 0.894\n",
      "2019-05-03T20:19:16.558210: Epoch   4 Batch 2280/3125   train_loss = 0.925\n",
      "2019-05-03T20:19:19.006045: Epoch   4 Batch 2300/3125   train_loss = 0.895\n",
      "2019-05-03T20:19:21.332206: Epoch   4 Batch 2320/3125   train_loss = 0.945\n",
      "2019-05-03T20:19:23.675754: Epoch   4 Batch 2340/3125   train_loss = 0.856\n",
      "2019-05-03T20:19:25.979039: Epoch   4 Batch 2360/3125   train_loss = 0.894\n",
      "2019-05-03T20:19:28.295346: Epoch   4 Batch 2380/3125   train_loss = 0.852\n",
      "2019-05-03T20:19:30.605165: Epoch   4 Batch 2400/3125   train_loss = 0.959\n",
      "2019-05-03T20:19:32.892946: Epoch   4 Batch 2420/3125   train_loss = 0.782\n",
      "2019-05-03T20:19:35.199970: Epoch   4 Batch 2440/3125   train_loss = 0.810\n",
      "2019-05-03T20:19:37.531303: Epoch   4 Batch 2460/3125   train_loss = 0.852\n",
      "2019-05-03T20:19:39.845695: Epoch   4 Batch 2480/3125   train_loss = 0.993\n",
      "2019-05-03T20:19:42.126625: Epoch   4 Batch 2500/3125   train_loss = 0.818\n",
      "2019-05-03T20:19:44.448627: Epoch   4 Batch 2520/3125   train_loss = 0.931\n",
      "2019-05-03T20:19:46.774761: Epoch   4 Batch 2540/3125   train_loss = 0.822\n",
      "2019-05-03T20:19:49.114647: Epoch   4 Batch 2560/3125   train_loss = 0.670\n",
      "2019-05-03T20:19:51.424225: Epoch   4 Batch 2580/3125   train_loss = 0.833\n",
      "2019-05-03T20:19:53.752210: Epoch   4 Batch 2600/3125   train_loss = 0.856\n",
      "2019-05-03T20:19:56.043807: Epoch   4 Batch 2620/3125   train_loss = 0.796\n",
      "2019-05-03T20:19:58.367696: Epoch   4 Batch 2640/3125   train_loss = 0.860\n",
      "2019-05-03T20:20:00.803277: Epoch   4 Batch 2660/3125   train_loss = 1.003\n",
      "2019-05-03T20:20:03.095313: Epoch   4 Batch 2680/3125   train_loss = 0.804\n",
      "2019-05-03T20:20:05.414532: Epoch   4 Batch 2700/3125   train_loss = 0.857\n",
      "2019-05-03T20:20:07.726550: Epoch   4 Batch 2720/3125   train_loss = 0.754\n",
      "2019-05-03T20:20:10.114602: Epoch   4 Batch 2740/3125   train_loss = 0.896\n",
      "2019-05-03T20:20:12.432793: Epoch   4 Batch 2760/3125   train_loss = 0.804\n",
      "2019-05-03T20:20:14.714684: Epoch   4 Batch 2780/3125   train_loss = 0.818\n",
      "2019-05-03T20:20:17.027957: Epoch   4 Batch 2800/3125   train_loss = 1.001\n",
      "2019-05-03T20:20:19.317719: Epoch   4 Batch 2820/3125   train_loss = 1.078\n",
      "2019-05-03T20:20:21.660476: Epoch   4 Batch 2840/3125   train_loss = 0.789\n",
      "2019-05-03T20:20:23.959497: Epoch   4 Batch 2860/3125   train_loss = 0.832\n",
      "2019-05-03T20:20:26.255341: Epoch   4 Batch 2880/3125   train_loss = 0.841\n",
      "2019-05-03T20:20:28.575295: Epoch   4 Batch 2900/3125   train_loss = 0.830\n",
      "2019-05-03T20:20:30.874643: Epoch   4 Batch 2920/3125   train_loss = 0.887\n",
      "2019-05-03T20:20:33.210371: Epoch   4 Batch 2940/3125   train_loss = 0.921\n",
      "2019-05-03T20:20:35.520743: Epoch   4 Batch 2960/3125   train_loss = 0.906\n",
      "2019-05-03T20:20:37.853896: Epoch   4 Batch 2980/3125   train_loss = 0.811\n",
      "2019-05-03T20:20:40.122364: Epoch   4 Batch 3000/3125   train_loss = 0.966\n",
      "2019-05-03T20:20:42.494938: Epoch   4 Batch 3020/3125   train_loss = 1.000\n",
      "2019-05-03T20:20:44.813643: Epoch   4 Batch 3040/3125   train_loss = 0.940\n",
      "2019-05-03T20:20:47.118748: Epoch   4 Batch 3060/3125   train_loss = 0.785\n",
      "2019-05-03T20:20:49.454940: Epoch   4 Batch 3080/3125   train_loss = 1.015\n",
      "2019-05-03T20:20:51.775585: Epoch   4 Batch 3100/3125   train_loss = 1.000\n",
      "2019-05-03T20:20:54.068170: Epoch   4 Batch 3120/3125   train_loss = 0.837\n",
      "2019-05-03T20:20:54.817408: Epoch   4 Batch   16/781   test_loss = 0.796\n",
      "2019-05-03T20:20:55.153491: Epoch   4 Batch   36/781   test_loss = 0.915\n",
      "2019-05-03T20:20:55.473972: Epoch   4 Batch   56/781   test_loss = 0.928\n",
      "2019-05-03T20:20:55.804983: Epoch   4 Batch   76/781   test_loss = 1.032\n",
      "2019-05-03T20:20:56.129599: Epoch   4 Batch   96/781   test_loss = 1.003\n",
      "2019-05-03T20:20:56.454053: Epoch   4 Batch  116/781   test_loss = 0.838\n",
      "2019-05-03T20:20:56.784496: Epoch   4 Batch  136/781   test_loss = 0.846\n",
      "2019-05-03T20:20:57.111098: Epoch   4 Batch  156/781   test_loss = 0.876\n",
      "2019-05-03T20:20:57.473550: Epoch   4 Batch  176/781   test_loss = 0.906\n",
      "2019-05-03T20:20:57.806781: Epoch   4 Batch  196/781   test_loss = 0.845\n",
      "2019-05-03T20:20:58.145702: Epoch   4 Batch  216/781   test_loss = 0.944\n",
      "2019-05-03T20:20:58.466320: Epoch   4 Batch  236/781   test_loss = 0.842\n",
      "2019-05-03T20:20:58.788186: Epoch   4 Batch  256/781   test_loss = 0.827\n",
      "2019-05-03T20:20:59.113427: Epoch   4 Batch  276/781   test_loss = 1.033\n",
      "2019-05-03T20:20:59.446812: Epoch   4 Batch  296/781   test_loss = 0.826\n",
      "2019-05-03T20:20:59.774528: Epoch   4 Batch  316/781   test_loss = 0.801\n",
      "2019-05-03T20:21:00.098081: Epoch   4 Batch  336/781   test_loss = 0.746\n",
      "2019-05-03T20:21:00.407742: Epoch   4 Batch  356/781   test_loss = 0.885\n",
      "2019-05-03T20:21:00.738197: Epoch   4 Batch  376/781   test_loss = 0.893\n",
      "2019-05-03T20:21:01.064095: Epoch   4 Batch  396/781   test_loss = 0.847\n",
      "2019-05-03T20:21:01.382701: Epoch   4 Batch  416/781   test_loss = 0.945\n",
      "2019-05-03T20:21:01.728295: Epoch   4 Batch  436/781   test_loss = 0.855\n",
      "2019-05-03T20:21:02.048062: Epoch   4 Batch  456/781   test_loss = 0.753\n",
      "2019-05-03T20:21:02.366299: Epoch   4 Batch  476/781   test_loss = 0.941\n",
      "2019-05-03T20:21:02.697724: Epoch   4 Batch  496/781   test_loss = 1.023\n",
      "2019-05-03T20:21:03.017991: Epoch   4 Batch  516/781   test_loss = 0.785\n",
      "2019-05-03T20:21:03.341058: Epoch   4 Batch  536/781   test_loss = 0.934\n",
      "2019-05-03T20:21:03.677194: Epoch   4 Batch  556/781   test_loss = 0.835\n",
      "2019-05-03T20:21:03.998671: Epoch   4 Batch  576/781   test_loss = 0.953\n",
      "2019-05-03T20:21:04.326145: Epoch   4 Batch  596/781   test_loss = 0.941\n",
      "2019-05-03T20:21:04.649637: Epoch   4 Batch  616/781   test_loss = 0.974\n",
      "2019-05-03T20:21:04.968181: Epoch   4 Batch  636/781   test_loss = 0.845\n",
      "2019-05-03T20:21:05.281290: Epoch   4 Batch  656/781   test_loss = 0.866\n",
      "2019-05-03T20:21:05.618181: Epoch   4 Batch  676/781   test_loss = 1.061\n",
      "2019-05-03T20:21:05.937464: Epoch   4 Batch  696/781   test_loss = 0.782\n",
      "2019-05-03T20:21:06.267483: Epoch   4 Batch  716/781   test_loss = 0.912\n",
      "2019-05-03T20:21:06.584999: Epoch   4 Batch  736/781   test_loss = 1.044\n",
      "2019-05-03T20:21:06.918631: Epoch   4 Batch  756/781   test_loss = 0.844\n",
      "2019-05-03T20:21:07.236350: Epoch   4 Batch  776/781   test_loss = 0.714\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "losses = {'train':[], 'test':[]}\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    \n",
    "    #搜集数据给tensorBoard用\n",
    "    # Keep track of gradient values and sparsity\n",
    "    grad_summaries = []\n",
    "    for g, v in gradients:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name.replace(':', '_')), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(':', '_')), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "        \n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "     \n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Inference summaries\n",
    "    inference_summary_op = tf.summary.merge([loss_summary])\n",
    "    inference_summary_dir = os.path.join(out_dir, \"summaries\", \"inference\")\n",
    "    inference_summary_writer = tf.summary.FileWriter(inference_summary_dir, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch_i in range(num_epochs):\n",
    "        \n",
    "        #将数据集分成训练集和测试集，随机种子不固定\n",
    "        train_X,test_X, train_y, test_y = train_test_split(features,  \n",
    "                                                           targets_values,  \n",
    "                                                           test_size = 0.2,  \n",
    "                                                           random_state = 0)  \n",
    "        \n",
    "        train_batches = get_batches(train_X, train_y, batch_size)\n",
    "        test_batches = get_batches(test_X, test_y, batch_size)\n",
    "    \n",
    "        #训练的迭代，保存训练损失\n",
    "        for batch_i in range(len(train_X) // batch_size):\n",
    "            x, y = next(train_batches)\n",
    "\n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: dropout_keep, #dropout_keep\n",
    "                lr: learning_rate}\n",
    "\n",
    "            step, train_loss, summaries, _ = sess.run([global_step, loss, train_summary_op, train_op], feed)  #cost\n",
    "            losses['train'].append(train_loss)\n",
    "            train_summary_writer.add_summary(summaries, step)  #\n",
    "            \n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * (len(train_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(train_X) // batch_size),\n",
    "                    train_loss))\n",
    "                \n",
    "        #使用测试数据的迭代\n",
    "        for batch_i  in range(len(test_X) // batch_size):\n",
    "            x, y = next(test_batches)\n",
    "            \n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: 1,\n",
    "                lr: learning_rate}\n",
    "            \n",
    "            step, test_loss, summaries = sess.run([global_step, loss, inference_summary_op], feed)  #cost\n",
    "\n",
    "            #保存测试损失\n",
    "            losses['test'].append(test_loss)\n",
    "            inference_summary_writer.add_summary(summaries, step)  #\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if (epoch_i * (len(test_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   test_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(test_X) // batch_size),\n",
    "                    test_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver.save(sess, save_dir)  #, global_step=epoch_i\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params((save_dir))\n",
    "\n",
    "load_dir = load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 377
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 377
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "\n",
    "    uid = loaded_graph.get_tensor_by_name(\"uid:0\")\n",
    "    user_gender = loaded_graph.get_tensor_by_name(\"user_gender:0\")\n",
    "    user_age = loaded_graph.get_tensor_by_name(\"user_age:0\")\n",
    "    user_job = loaded_graph.get_tensor_by_name(\"user_job:0\")\n",
    "    movie_id = loaded_graph.get_tensor_by_name(\"movie_id:0\")\n",
    "    movie_categories = loaded_graph.get_tensor_by_name(\"movie_categories:0\")\n",
    "    movie_titles = loaded_graph.get_tensor_by_name(\"movie_titles:0\")\n",
    "    targets = loaded_graph.get_tensor_by_name(\"targets:0\")\n",
    "    dropout_keep_prob = loaded_graph.get_tensor_by_name(\"dropout_keep_prob:0\")\n",
    "    lr = loaded_graph.get_tensor_by_name(\"LearningRate:0\")\n",
    "    #两种不同计算预测评分的方案使用不同的name获取tensor inference\n",
    "#     inference = loaded_graph.get_tensor_by_name(\"inference/inference/BiasAdd:0\")\n",
    "    inference = loaded_graph.get_tensor_by_name(\"inference/ExpandDims:0\") # 之前是MatMul:0 因为inference代码修改了 这里也要修改 感谢网友 @清歌 指出问题\n",
    "    movie_combine_layer_flat = loaded_graph.get_tensor_by_name(\"movie_fc/Reshape:0\")\n",
    "    user_combine_layer_flat = loaded_graph.get_tensor_by_name(\"user_fc/Reshape:0\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference, movie_combine_layer_flat, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_movie(user_id_val, movie_id_val):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "    \n",
    "        # Get Tensors from loaded model\n",
    "        uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference,_, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "    \n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = movies.values[movieid2idx[movie_id_val]][2]\n",
    "    \n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = movies.values[movieid2idx[movie_id_val]][1]\n",
    "    \n",
    "        feed = {\n",
    "              uid: np.reshape(users.values[user_id_val-1][0], [1, 1]),\n",
    "              user_gender: np.reshape(users.values[user_id_val-1][1], [1, 1]),\n",
    "              user_age: np.reshape(users.values[user_id_val-1][2], [1, 1]),\n",
    "              user_job: np.reshape(users.values[user_id_val-1][3], [1, 1]),\n",
    "              movie_id: np.reshape(movies.values[movieid2idx[movie_id_val]][0], [1, 1]),\n",
    "              movie_categories: categories,  #x.take(6,1)\n",
    "              movie_titles: titles,  #x.take(5,1)\n",
    "              dropout_keep_prob: 1}\n",
    "    \n",
    "        # Get Prediction\n",
    "        inference_val = sess.run([inference], feed)  \n",
    "    \n",
    "        return (inference_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[4.282253]], dtype=float32)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_movie(234, 1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "movie_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, movie_combine_layer_flat, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in movies.values:\n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = item.take(2)\n",
    "\n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = item.take(1)\n",
    "\n",
    "        feed = {\n",
    "            movie_id: np.reshape(item.take(0), [1, 1]),\n",
    "            movie_categories: categories,  #x.take(6,1)\n",
    "            movie_titles: titles,  #x.take(5,1)\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        movie_combine_layer_flat_val = sess.run([movie_combine_layer_flat], feed)  \n",
    "        movie_matrics.append(movie_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(movie_matrics).reshape(-1, 200)), open('movie_matrics.p', 'wb'))\n",
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "users_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, __,user_combine_layer_flat = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in users.values:\n",
    "\n",
    "        feed = {\n",
    "            uid: np.reshape(item.take(0), [1, 1]),\n",
    "            user_gender: np.reshape(item.take(1), [1, 1]),\n",
    "            user_age: np.reshape(item.take(2), [1, 1]),\n",
    "            user_job: np.reshape(item.take(3), [1, 1]),\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        user_combine_layer_flat_val = sess.run([user_combine_layer_flat], feed)  \n",
    "        users_matrics.append(user_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(users_matrics).reshape(-1, 200)), open('users_matrics.p', 'wb'))\n",
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_same_type_movie(movie_id_val, top_k = 20):\n",
    "    \n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "        \n",
    "        norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keep_dims=True))\n",
    "        normalized_movie_matrics = movie_matrics / norm_movie_matrics\n",
    "\n",
    "        #推荐同类型的电影\n",
    "        probs_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalized_movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "        \n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "您看的电影是：[1400 'Somebody is Waiting (1996)' 'Drama']\n",
      "以下是给您的推荐：\n",
      "1083\n",
      "[1099 'Christmas Carol, A (1938)' 'Drama']\n",
      "1518\n",
      "[1557 'Squeeze (1996)' 'Drama']\n",
      "3511\n",
      "[3580 'Up at the Villa (2000)' 'Drama']\n",
      "1273\n",
      "[1293 'Gandhi (1982)' 'Drama']\n",
      "1883\n",
      "[1952 'Midnight Cowboy (1969)' 'Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1083, 1273, 1518, 1883, 3511}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_same_type_movie(1400, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_your_favorite_movie(user_id_val, top_k = 10):\n",
    "\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        #推荐您喜欢的电影\n",
    "        probs_embeddings = (users_matrics[user_id_val-1]).reshape([1, 200])\n",
    "\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     print(sim.shape)\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "        \n",
    "    #     sim_norm = probs_norm_similarity.eval()\n",
    "    #     print((-sim_norm[0]).argsort()[0:top_k])\n",
    "    \n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "以下是给您的推荐：\n",
      "711\n",
      "[720 'Wallace & Gromit: The Best of Aardman Animation (1996)' 'Animation']\n",
      "910\n",
      "[922 'Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)' 'Film-Noir']\n",
      "2255\n",
      "[2324 'Life Is Beautiful (La Vita � bella) (1997)' 'Comedy|Drama']\n",
      "2840\n",
      "[2909 'Five Wives, Three Secretaries and Me (1998)' 'Documentary']\n",
      "315\n",
      "[318 'Shawshank Redemption, The (1994)' 'Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{315, 711, 910, 2255, 2840}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_your_favorite_movie(234, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def recommend_other_favorite_movie(movie_id_val, top_k = 20):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        probs_movie_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_user_favorite_similarity = tf.matmul(probs_movie_embeddings, tf.transpose(users_matrics))\n",
    "        favorite_user_id = np.argsort(probs_user_favorite_similarity.eval())[0][-top_k:]\n",
    "    #     print(normalized_users_matrics.eval().shape)\n",
    "    #     print(probs_user_favorite_similarity.eval()[0][favorite_user_id])\n",
    "    #     print(favorite_user_id.shape)\n",
    "    \n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        \n",
    "        print(\"喜欢看这个电影的人是：{}\".format(users_orig[favorite_user_id-1]))\n",
    "        probs_users_embeddings = (users_matrics[favorite_user_id-1]).reshape([-1, 200])\n",
    "        probs_similarity = tf.matmul(probs_users_embeddings, tf.transpose(movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "    \n",
    "    #     print(sim.shape)\n",
    "    #     print(np.argmax(sim, 1))\n",
    "        p = np.argmax(sim, 1)\n",
    "        print(\"喜欢看这个电影的人还喜欢看：\")\n",
    "\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = p[random.randrange(top_k)]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "您看的电影是：[1400 'Somebody is Waiting (1996)' 'Drama']\n",
      "喜欢看这个电影的人是：[[35 'M' 45 1]\n",
      " [2587 'M' 35 7]\n",
      " [3364 'M' 56 13]\n",
      " [2496 'M' 50 1]\n",
      " [3901 'M' 18 14]\n",
      " [4557 'M' 25 14]\n",
      " [763 'M' 18 10]\n",
      " [5084 'M' 18 4]\n",
      " [4314 'M' 25 17]\n",
      " [1745 'M' 45 0]\n",
      " [3833 'M' 25 1]\n",
      " [2639 'F' 25 6]\n",
      " [4067 'M' 45 20]\n",
      " [1982 'M' 1 10]\n",
      " [2899 'M' 35 14]\n",
      " [5335 'M' 25 14]\n",
      " [1763 'M' 35 7]\n",
      " [3031 'M' 18 4]\n",
      " [4903 'M' 35 12]\n",
      " [5068 'M' 35 7]]\n",
      "喜欢看这个电影的人还喜欢看：\n",
      "714\n",
      "[723 'Two Friends (1986)' 'Drama']\n",
      "49\n",
      "[50 'Usual Suspects, The (1995)' 'Crime|Thriller']\n",
      "2840\n",
      "[2909 'Five Wives, Three Secretaries and Me (1998)' 'Documentary']\n",
      "1950\n",
      "[2019\n",
      " 'Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)'\n",
      " 'Action|Drama']\n",
      "735\n",
      "[745 'Close Shave, A (1995)' 'Animation|Comedy|Thriller']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{49, 714, 735, 1950, 2840}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_other_favorite_movie(1400, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
